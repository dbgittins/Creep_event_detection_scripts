{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88548d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import matplotlib.patches as patches\n",
    "import cmcrameri.cm as cmc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from geopy.distance import geodesic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4342be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/56/47k8q3q1449cq8kwzkz5r8ch0000gr/T/ipykernel_12026/2006262863.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  creepmeters_picks['Start Time'] = pd.to_datetime(creepmeters_picks['Start Time'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['EAF', 'PARK', 'UTA', 'NAF', 'HAY', 'CHAF', 'HOL', 'DSF', 'SOCAL',\n",
       "       'RID', 'CAL'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creepmeters = pd.read_csv('../../Data/DATA_tidied/creepmeter_metadata_post_standardisation_sac_codes_updated.csv')\n",
    "creepmeters.drop_duplicates('Creepmeter_abbrv',inplace=True)\n",
    "creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='XMBC'].index,inplace=True)\n",
    "creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='TABC'].index,inplace=True)\n",
    "creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='CHP1'].index,inplace=True)\n",
    "#creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='CTM1'].index,inplace=True)\n",
    "#creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='XHR3'].index,inplace=True)\n",
    "#creepmeters.drop(creepmeters[creepmeters['Creepmeter_abbrv']=='SH30'].index,inplace=True) #needs checking as defintely picks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "creepmeters.reset_index(inplace=True,drop=True)\n",
    "creepmeters_picks = pd.read_csv('../../Data/DATA_tidied/Picks/All_picks_29_May_2025.csv',index_col=0)\n",
    "creepmeters_picks['Start Time'] = pd.to_datetime(creepmeters_picks['Start Time'])\n",
    "creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Start Time']>dt.datetime(2024,1,1,0,0,0)].index,inplace=True)\n",
    "creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Displacement, mm']<0.02].index,inplace=True)\n",
    "networks = creepmeters['Network'].unique()\n",
    "networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824bd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9b9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epicentral_distance_and_travel_time(row):\n",
    "    \"\"\"\n",
    "    Computes the epicentral distance and surface wave travel time between\n",
    "    an earthquake and a creepmeter.\n",
    "\n",
    "    Parameters:\n",
    "        row: A row from the nearest_eq_df DataFrame, containing earthquake and creepmeter coordinates.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with:\n",
    "        - Epicentral distance in kilometres\n",
    "        - Estimated surface wave travel time in seconds (assuming 3.5 km/s)\n",
    "    \"\"\"\n",
    "    epicentral_distance_km = geodesic(\n",
    "        (row['Latitude_creep'], row['Longitude_creep']),\n",
    "        (row['latitude'], row['longitude'])\n",
    "    ).km\n",
    "    surface_wave_speed_km_s = 3.5  # km/s for Rayleigh/Love waves in crust\n",
    "    travel_time_s = epicentral_distance_km / surface_wave_speed_km_s\n",
    "    return epicentral_distance_km, travel_time_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72092382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe9f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAF\n",
      "BAL1\n",
      "BAL1 37.9903 38.199\n",
      "0.00% of 2 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "GOK1\n",
      "GOK1 38.006 36.5267\n",
      "0.00% of 130 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "GOZ1\n",
      "GOZ1 38.1759 38.0103\n",
      "0.00% of 1 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "HAS1\n",
      "HAS1 36.8006 36.5185\n",
      "0.00% of 8 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "HAT1\n",
      "HAT1 38.387 36.2803\n",
      "0.00% of 7 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "KAR1\n",
      "KIR1\n",
      "KIR1 36.4791 36.3339\n",
      "0.00% of 6 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "ORM1\n",
      "ORM1 38.2113 38.7732\n",
      "0.00% of 5 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PAN1\n",
      "PAN1 38.699 39.9537\n",
      "5.00% of 20 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PAS1\n",
      "PAS1 38.699 39.9537\n",
      "0.00% of 16 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "SIV1\n",
      "SIV1 38.388 39.1873\n",
      "0.00% of 112 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "TAS1\n",
      "TAS1 38.2049 38.7872\n",
      "0.00% of 4 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "YZE1\n",
      "YZE1 38.1791 38.7526\n",
      "0.00% of 1 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "YZW1\n",
      "YZW1 38.1086 38.7361\n",
      "0.00% of 10 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PARK\n",
      "C461\n",
      "C461 35.724 -120.282\n",
      "0.00% of 43 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "C462\n",
      "C462 35.724 -120.282\n",
      "0.00% of 24 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "CRR1\n",
      "CRR1 35.835 -120.363\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m best_time_diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, eq \u001b[38;5;129;01min\u001b[39;00m eq_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 92\u001b[0m     dist_km \u001b[38;5;241m=\u001b[39m \u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43meq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkm\n\u001b[1;32m     93\u001b[0m     travel_time \u001b[38;5;241m=\u001b[39m dist_km \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3.5\u001b[39m\n\u001b[1;32m     94\u001b[0m     time_since_eq \u001b[38;5;241m=\u001b[39m (start_time \u001b[38;5;241m-\u001b[39m eq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meq_time\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/geopy/distance.py:540\u001b[0m, in \u001b[0;36mgeodesic.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ellipsoid(kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mellipsoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWGS-84\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    539\u001b[0m major, minor, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID\n\u001b[0;32m--> 540\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/geopy/distance.py:276\u001b[0m, in \u001b[0;36mDistance.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m util\u001b[38;5;241m.\u001b[39mpairwise(args):\n\u001b[0;32m--> 276\u001b[0m         kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m kilometers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m units\u001b[38;5;241m.\u001b[39mkilometers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kilometers \u001b[38;5;241m=\u001b[39m kilometers\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/geopy/distance.py:564\u001b[0m, in \u001b[0;36mgeodesic.measure\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    559\u001b[0m lat2, lon2 \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mlatitude, b\u001b[38;5;241m.\u001b[39mlongitude\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod, Geodesic) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39ma \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mELLIPSOID[\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod \u001b[38;5;241m=\u001b[39m \u001b[43mGeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELLIPSOID\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELLIPSOID\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m s12 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeod\u001b[38;5;241m.\u001b[39mInverse(lat1, lon1, lat2, lon2,\n\u001b[1;32m    567\u001b[0m                         Geodesic\u001b[38;5;241m.\u001b[39mDISTANCE)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms12\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s12\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/geographiclib/geodesic.py:318\u001b[0m, in \u001b[0;36mGeodesic.__init__\u001b[0;34m(self, a, f)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C3x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(Geodesic\u001b[38;5;241m.\u001b[39mnC3x_))\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C4x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(Geodesic\u001b[38;5;241m.\u001b[39mnC4x_))\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_A3coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C3coeff()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_C4coeff()\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/geographiclib/geodesic.py:335\u001b[0m, in \u001b[0;36mGeodesic._A3coeff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Geodesic\u001b[38;5;241m.\u001b[39mnA3_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m# coeff of eps^j\u001b[39;00m\n\u001b[1;32m    334\u001b[0m   m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(Geodesic\u001b[38;5;241m.\u001b[39mnA3_ \u001b[38;5;241m-\u001b[39m j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, j) \u001b[38;5;66;03m# order of polynomial in n\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A3x[k] \u001b[38;5;241m=\u001b[39m \u001b[43mMath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolyval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m coeff[o \u001b[38;5;241m+\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    336\u001b[0m   k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    337\u001b[0m   o \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Define parameters\n",
    "radius_km = 1000\n",
    "maxradius_deg = radius_km / 111.0\n",
    "min_magnitude = 4  # Set minimum magnitude for EQ search\n",
    "\n",
    "# Containers to store all output across networks\n",
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all = pd.DataFrame()\n",
    "\n",
    "# Loop over each network in your list\n",
    "for j in range(len(networks)):\n",
    "    print(networks[j])\n",
    "\n",
    "    # Limit processing to specific networks of interest\n",
    "    if networks[j] in ['EAF', 'PARK', 'UTA', 'NAF', 'HAY', 'CHAF', 'HOL', 'DSF', 'SOCAL', 'RID', 'CAL']:\n",
    "\n",
    "        # Filter creepmeters to just those in this network\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network'] != networks[j]].index)\n",
    "\n",
    "        # Loop over each creepmeter in the network\n",
    "        for i in range(len(creepmeters_network)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]\n",
    "            print(abbrv)\n",
    "\n",
    "            # Filter picks (creep events) for this creepmeter\n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation'] != abbrv].index)\n",
    "\n",
    "            # Skip if no picks for this creepmeter\n",
    "            if picks.empty:\n",
    "                continue\n",
    "\n",
    "            # Get creepmeter location\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv, lat, long)\n",
    "\n",
    "            # Extract per-event sampling interval (in seconds)\n",
    "            sampling_freq = picks['Sampling rate, s']\n",
    "\n",
    "            # Determine query time window from picks\n",
    "            starttime = UTCDateTime((pd.to_datetime(picks['Start Time']).min() - pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "            endtime = UTCDateTime((pd.to_datetime(picks['Start Time']).max() + pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "            # Query IRIS FDSN event service\n",
    "            client = Client(\"IRIS\")\n",
    "            try:\n",
    "                catalog = client.get_events(\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime,\n",
    "                    latitude=lat,\n",
    "                    longitude=long,\n",
    "                    maxradius=maxradius_deg,  # ~500 km\n",
    "                    minmagnitude=min_magnitude\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve events for {abbrv}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Parse earthquake metadata into DataFrame\n",
    "            eq_data = []\n",
    "            for event in catalog:\n",
    "                origin = event.preferred_origin() or event.origins[0]\n",
    "                magnitude = event.preferred_magnitude() or event.magnitudes[0]\n",
    "                eq_data.append({\n",
    "                    'eq_time': origin.time.datetime,\n",
    "                    'magnitude': magnitude.mag,\n",
    "                    'latitude': origin.latitude,\n",
    "                    'longitude': origin.longitude,\n",
    "                    'depth_km': origin.depth / 1000.0,\n",
    "                    'place': event.resource_id.id.split('/')[-1]\n",
    "                })\n",
    "\n",
    "            eq_df = pd.DataFrame(eq_data)\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            eq_df['eq_time'] = pd.to_datetime(eq_df['eq_time'])\n",
    "\n",
    "            # Search for plausible EQ triggers based on wave arrival time\n",
    "            expanded_matches = []\n",
    "            for idx, pick in picks.iterrows():\n",
    "                start_time = pick['Start Time']\n",
    "                sampling_interval = pick['Sampling rate, s']\n",
    "\n",
    "                valid_trigger_found = False\n",
    "                best_match = None\n",
    "                best_time_diff = np.inf\n",
    "\n",
    "                for _, eq in eq_df.iterrows():\n",
    "                    dist_km = geodesic((lat, long), (eq['latitude'], eq['longitude'])).km\n",
    "                    travel_time = dist_km / 3.5\n",
    "                    time_since_eq = (start_time - eq['eq_time']).total_seconds()\n",
    "                    upper_bound = travel_time + 2 * sampling_interval + 1.0 * sampling_interval + 60\n",
    "\n",
    "                    if travel_time <= time_since_eq <= upper_bound:\n",
    "                        if time_since_eq < best_time_diff:\n",
    "                            valid_trigger_found = True\n",
    "                            best_time_diff = time_since_eq\n",
    "                            best_match = {\n",
    "                                'Start Time': start_time,\n",
    "                                'abbrv': abbrv,\n",
    "                                'eq_time': eq['eq_time'],\n",
    "                                'magnitude': eq['magnitude'],\n",
    "                                'latitude': eq['latitude'],\n",
    "                                'longitude': eq['longitude'],\n",
    "                                'depth_km': eq['depth_km'],\n",
    "                                'place': eq['place'],\n",
    "                                'Time Since EQ [s]': time_since_eq,\n",
    "                                'Surface Wave Travel Time [s]': travel_time,\n",
    "                                'Upper Tolerance Bound [s]': upper_bound,\n",
    "                                'Likely Triggered': True,\n",
    "                                'Sampling Interval [s]': sampling_interval,\n",
    "                                'Latitude_creep': lat,\n",
    "                                'Longitude_creep': long\n",
    "                            }\n",
    "\n",
    "                if not valid_trigger_found:\n",
    "                    best_match = {\n",
    "                        'Start Time': start_time,\n",
    "                        'abbrv': abbrv,\n",
    "                        'eq_time': pd.NaT,\n",
    "                        'magnitude': np.nan,\n",
    "                        'latitude': np.nan,\n",
    "                        'longitude': np.nan,\n",
    "                        'depth_km': np.nan,\n",
    "                        'place': None,\n",
    "                        'Time Since EQ [s]': np.nan,\n",
    "                        'Surface Wave Travel Time [s]': np.nan,\n",
    "                        'Upper Tolerance Bound [s]': np.nan,\n",
    "                        'Likely Triggered': False,\n",
    "                        'Sampling Interval [s]': sampling_interval,\n",
    "                        'Latitude_creep': lat,\n",
    "                        'Longitude_creep': long\n",
    "                    }\n",
    "\n",
    "                expanded_matches.append(best_match)\n",
    "\n",
    "            nearest_eq_df = pd.DataFrame(expanded_matches)\n",
    "\n",
    "            # Compute percentage of events considered valid triggers\n",
    "            count_valid = nearest_eq_df['Likely Triggered'].sum()\n",
    "            percent_valid = 100 * count_valid / len(picks)\n",
    "\n",
    "            print(f\"{percent_valid:.2f}% of {len(picks)} events have valid earthquake triggers (wave arrival + resolution buffer).\")\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({\n",
    "                'Creepmeter': [abbrv],\n",
    "                'percentage valid triggers': [round(percent_valid, 2)],\n",
    "                'Latitude': lat\n",
    "            })\n",
    "\n",
    "            creepmeter_eq_picks = pd.concat([picks.reset_index(drop=True), nearest_eq_df.reset_index(drop=True)], axis=1)\n",
    "            my_data = pd.concat([my_data, creepmeter_eq_picks])\n",
    "            percentage_triggered_all = pd.concat([percentage_triggered_all, percentage_triggered])\n",
    "\n",
    "        percentage_triggered_all.sort_values(by='Latitude', inplace=True, ascending=False)\n",
    "        percentage_triggered_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "my_data.to_csv(\"triggered_creep_events_matched_to_nearest_EQ.csv\", index=False)\n",
    "percentage_triggered_all.to_csv(\"triggered_creep_percentage_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f02733c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Network</th>\n",
       "      <th>Creepmeter full name</th>\n",
       "      <th>Creepmeter abbreviation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Sampling rate, s</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Displacement, mm</th>\n",
       "      <th>Duration, hrs</th>\n",
       "      <th>Maximum velocity, m/s</th>\n",
       "      <th>Event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1989-03-23 01:00:00</td>\n",
       "      <td>24/3/89 16:50</td>\n",
       "      <td>1.53</td>\n",
       "      <td>39.833333</td>\n",
       "      <td>6.666667e-08</td>\n",
       "      <td>0280_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1989-03-24 17:30:00</td>\n",
       "      <td>26/3/89 18:49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>49.333056</td>\n",
       "      <td>3.333333e-08</td>\n",
       "      <td>0281_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1991-03-19 01:09:00</td>\n",
       "      <td>19/3/91 16:39</td>\n",
       "      <td>0.21</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>3.333333e-08</td>\n",
       "      <td>0425_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1991-03-20 07:50:00</td>\n",
       "      <td>21/3/91 04:59</td>\n",
       "      <td>0.14</td>\n",
       "      <td>21.166389</td>\n",
       "      <td>6.666667e-08</td>\n",
       "      <td>0430_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1992-02-13 23:39:00</td>\n",
       "      <td>15/2/92 16:20</td>\n",
       "      <td>0.29</td>\n",
       "      <td>40.666944</td>\n",
       "      <td>3.333333e-08</td>\n",
       "      <td>0536_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2011-12-25 00:20:00</td>\n",
       "      <td>25/12/11 17:19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>16.999722</td>\n",
       "      <td>3.333333e-08</td>\n",
       "      <td>3440_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2012-01-01 21:00:00</td>\n",
       "      <td>3/1/12 11:10</td>\n",
       "      <td>0.85</td>\n",
       "      <td>38.166667</td>\n",
       "      <td>1.500000e-07</td>\n",
       "      <td>3444_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2013-02-14 00:49:00</td>\n",
       "      <td>18/2/13 19:00</td>\n",
       "      <td>2.43</td>\n",
       "      <td>114.166944</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3564_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2014-01-25 09:00:00</td>\n",
       "      <td>26/1/14 17:30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>8.333333e-08</td>\n",
       "      <td>3659_CRR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>PARK</td>\n",
       "      <td>Carr Ranch</td>\n",
       "      <td>CRR1</td>\n",
       "      <td>35.835</td>\n",
       "      <td>-120.363</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2014-01-26 20:30:00</td>\n",
       "      <td>27/1/14 20:50</td>\n",
       "      <td>1.29</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3660_CRR1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Network Creepmeter full name Creepmeter abbreviation  Latitude  \\\n",
       "280     PARK           Carr Ranch                    CRR1    35.835   \n",
       "281     PARK           Carr Ranch                    CRR1    35.835   \n",
       "425     PARK           Carr Ranch                    CRR1    35.835   \n",
       "430     PARK           Carr Ranch                    CRR1    35.835   \n",
       "536     PARK           Carr Ranch                    CRR1    35.835   \n",
       "...      ...                  ...                     ...       ...   \n",
       "3440    PARK           Carr Ranch                    CRR1    35.835   \n",
       "3444    PARK           Carr Ranch                    CRR1    35.835   \n",
       "3564    PARK           Carr Ranch                    CRR1    35.835   \n",
       "3659    PARK           Carr Ranch                    CRR1    35.835   \n",
       "3660    PARK           Carr Ranch                    CRR1    35.835   \n",
       "\n",
       "      Longitude  Sampling rate, s          Start Time        End Time  \\\n",
       "280    -120.363             600.0 1989-03-23 01:00:00   24/3/89 16:50   \n",
       "281    -120.363             600.0 1989-03-24 17:30:00   26/3/89 18:49   \n",
       "425    -120.363             600.0 1991-03-19 01:09:00   19/3/91 16:39   \n",
       "430    -120.363             600.0 1991-03-20 07:50:00   21/3/91 04:59   \n",
       "536    -120.363             600.0 1992-02-13 23:39:00   15/2/92 16:20   \n",
       "...         ...               ...                 ...             ...   \n",
       "3440   -120.363             600.0 2011-12-25 00:20:00  25/12/11 17:19   \n",
       "3444   -120.363             600.0 2012-01-01 21:00:00    3/1/12 11:10   \n",
       "3564   -120.363             600.0 2013-02-14 00:49:00   18/2/13 19:00   \n",
       "3659   -120.363             600.0 2014-01-25 09:00:00   26/1/14 17:30   \n",
       "3660   -120.363             600.0 2014-01-26 20:30:00   27/1/14 20:50   \n",
       "\n",
       "      Displacement, mm  Duration, hrs  Maximum velocity, m/s   Event_id  \n",
       "280               1.53      39.833333           6.666667e-08  0280_CRR1  \n",
       "281               0.56      49.333056           3.333333e-08  0281_CRR1  \n",
       "425               0.21      15.500000           3.333333e-08  0425_CRR1  \n",
       "430               0.14      21.166389           6.666667e-08  0430_CRR1  \n",
       "536               0.29      40.666944           3.333333e-08  0536_CRR1  \n",
       "...                ...            ...                    ...        ...  \n",
       "3440              0.27      16.999722           3.333333e-08  3440_CRR1  \n",
       "3444              0.85      38.166667           1.500000e-07  3444_CRR1  \n",
       "3564              2.43     114.166944           1.000000e-07  3564_CRR1  \n",
       "3659              0.73      32.500000           8.333333e-08  3659_CRR1  \n",
       "3660              1.29      24.333333           1.000000e-07  3660_CRR1  \n",
       "\n",
       "[139 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6846aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAF\n",
      "BAL1\n",
      "BAL1 37.9903 38.199\n",
      "0.00% of 2 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "GOK1\n",
      "GOK1 38.006 36.5267\n",
      "0.00% of 130 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "GOZ1\n",
      "GOZ1 38.1759 38.0103\n",
      "0.00% of 1 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "HAS1\n",
      "HAS1 36.8006 36.5185\n",
      "0.00% of 8 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "HAT1\n",
      "HAT1 38.387 36.2803\n",
      "0.00% of 7 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "KAR1\n",
      "KIR1\n",
      "KIR1 36.4791 36.3339\n",
      "0.00% of 6 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "ORM1\n",
      "ORM1 38.2113 38.7732\n",
      "0.00% of 5 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PAN1\n",
      "PAN1 38.699 39.9537\n",
      "Failed to download waveform for 4700_PAN1: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "5.00% of 20 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PAS1\n",
      "PAS1 38.699 39.9537\n",
      "0.00% of 16 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "SIV1\n",
      "SIV1 38.388 39.1873\n",
      "0.00% of 112 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "TAS1\n",
      "TAS1 38.2049 38.7872\n",
      "0.00% of 4 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "YZE1\n",
      "YZE1 38.1791 38.7526\n",
      "0.00% of 1 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "YZW1\n",
      "YZW1 38.1086 38.7361\n",
      "0.00% of 10 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "PARK\n",
      "C461\n",
      "C461 35.724 -120.282\n",
      "0.00% of 43 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "C462\n",
      "C462 35.724 -120.282\n",
      "0.00% of 24 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "CRR1\n",
      "CRR1 35.835 -120.363\n",
      "0.00% of 139 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "WKR1\n",
      "WKR1 35.858 -120.392\n",
      "Trying station YH.CRAK (16.0 km) for all BH? channels\n",
      "Saved waveform to waveforms/2503_WKR1.mseed\n",
      "0.63% of 158 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "X461\n",
      "X461 35.723 -120.278\n",
      "Trying station BK.PKD (34.3 km) for all BH? channels\n",
      "Saved waveform to waveforms/2073_X461.mseed\n",
      "Trying station BK.PKD (34.3 km) for all BH? channels\n",
      "Saved waveform to waveforms/2075_X461.mseed\n",
      "1.37% of 146 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XGH1\n",
      "XGH1 35.82 -120.348\n",
      "0.00% of 88 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XHSW\n",
      "XHSW 35.862 -120.42\n",
      "0.00% of 69 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XMD1\n",
      "XMD1 35.943 -120.485\n",
      "0.00% of 449 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XMM1\n",
      "XMM1 35.958 -120.502\n",
      "Trying station G.SCZ (107.8 km) for all BH? channels\n",
      "Saved waveform to waveforms/0573_XMM1.mseed\n",
      "Trying station BK.PKD1 (10.2 km) for all BH? channels\n",
      "Station BK.PKD1 failed: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Trying station G.SCZ (107.8 km) for all BH? channels\n",
      "Station G.SCZ failed: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "All station attempts failed for event 0958_XMM1.\n",
      "0.19% of 1044 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XPK1\n",
      "XPK1 35.902 -120.442\n",
      "0.00% of 135 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XPK2\n",
      "XPK2 35.902 -120.442\n",
      "0.00% of 138 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XRSW\n",
      "XRSW 35.907 -120.46\n",
      "Trying station BK.PKD (8.5 km) for all BH? channels\n",
      "Saved waveform to waveforms/2072_XRSW.mseed\n",
      "2.70% of 37 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XSC1\n",
      "XSC1 36.065 -120.628\n",
      "0.00% of 91 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XTA1\n",
      "XTA1 35.89 -120.427\n",
      "Failed to download waveform for 0594_XTA1: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "0.49% of 203 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XVA1\n",
      "XVA1 35.922 -120.462\n",
      "Failed to download waveform for 0327_XVA1: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "0.32% of 315 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "UTA\n",
      "CAN1\n",
      "CAN1 38.0198 -110.0374\n",
      "0.00% of 6 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "NAF\n",
      "CER1\n",
      "CER1 40.8947 32.7773\n",
      "0.00% of 18 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "CHE1\n",
      "CHE1 40.8701 32.6285\n",
      "0.00% of 93 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "ESZ1\n",
      "ESZ1 40.8697 32.6209\n",
      "0.00% of 31 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "HAM1\n",
      "HAM1 40.8729 32.6603\n",
      "0.00% of 52 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "INW1\n",
      "INW1 40.8698 32.6258\n",
      "0.00% of 91 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "ISW1\n",
      "ISW1 40.8697 32.6258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 120\u001b[0m\n\u001b[1;32m    118\u001b[0m client \u001b[38;5;241m=\u001b[39m Client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRIS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     catalog \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarttime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxradius_deg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ~500 km\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminmagnitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_magnitude\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve events for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabbrv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:550\u001b[0m, in \u001b[0;36mClient.get_events\u001b[0;34m(self, starttime, endtime, minlatitude, maxlatitude, minlongitude, maxlongitude, latitude, longitude, minradius, maxradius, mindepth, maxdepth, minmagnitude, maxmagnitude, magnitudetype, eventtype, includeallorigins, includeallmagnitudes, includearrivals, eventid, limit, offset, orderby, catalog, contributor, updatedafter, filename, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m setup_query_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m, locs, kwargs)\n\u001b[1;32m    547\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_url_from_parameters(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_PARAMETERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs)\n\u001b[0;32m--> 550\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1482\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_type:\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[0;32m-> 1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_opener\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gzip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gzip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m raise_on_error(code, data)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:1958\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, opener, timeout, headers, debug, return_string, data, use_gzip)\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[38;5;66;03m# Cannot directly stream to gzip from urllib!\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;66;03m# http://www.enricozini.org/2011/cazzeggio/python-gzip/\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43murl_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m   1960\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProblem retrieving data from datacenter. \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/http/client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/http/client.py:583\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m         chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/http/client.py:566\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/http/client.py:526\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameters\n",
    "radius_km = 500\n",
    "maxradius_deg = radius_km / 111.0\n",
    "min_magnitude = 4  # Set minimum magnitude for EQ search\n",
    "\n",
    "# Channels to try\n",
    "channels_to_try = [\"BHZ\", \"BHE\", \"BHN\"]\n",
    "\n",
    "# Containers to store all output across networks\n",
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all = pd.DataFrame()\n",
    "\n",
    "# Function to download waveform from closest broadband station with fallback\n",
    "\n",
    "def download_waveform_for_event(event_time, eq_lat, eq_lon, event_id, creep_lat, creep_lon,\n",
    "                                 duration=300, save_folder=\"waveforms\", radius_deg=1.0):\n",
    "    client = Client(\"IRIS\")\n",
    "    starttime = UTCDateTime(event_time)\n",
    "    endtime = starttime + duration\n",
    "\n",
    "    try:\n",
    "        inventory = client.get_stations(\n",
    "            latitude=creep_lat,\n",
    "            longitude=creep_lon,\n",
    "            maxradius=radius_deg,\n",
    "            channel=\"BH?\",\n",
    "            level=\"channel\",\n",
    "            starttime=starttime,\n",
    "            endtime=endtime\n",
    "        )\n",
    "\n",
    "        if not inventory or len(inventory) == 0:\n",
    "            print(f\"No stations found near {event_id}\")\n",
    "            return\n",
    "\n",
    "        # Create list of candidate stations sorted by distance\n",
    "        station_list = []\n",
    "        for net in inventory:\n",
    "            for sta in net:\n",
    "                sta_lat = sta.latitude\n",
    "                sta_lon = sta.longitude\n",
    "                dist_km = geodesic((creep_lat, creep_lon), (sta_lat, sta_lon)).km\n",
    "                station_list.append({\n",
    "                    \"distance\": dist_km,\n",
    "                    \"network\": net.code,\n",
    "                    \"station\": sta.code,\n",
    "                    \"location\": \"*\"\n",
    "                })\n",
    "\n",
    "        station_list.sort(key=lambda x: x[\"distance\"])\n",
    "\n",
    "        # Try stations in order of proximity and download all BH? channels from the first working station\n",
    "        for sta in station_list:\n",
    "            try:\n",
    "                print(f\"Trying station {sta['network']}.{sta['station']} ({sta['distance']:.1f} km) for all BH? channels\")\n",
    "                st = client.get_waveforms(\n",
    "                    network=sta[\"network\"],\n",
    "                    station=sta[\"station\"],\n",
    "                    location=sta[\"location\"],\n",
    "                    channel=\"BH?\",\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime\n",
    "                )\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                output_path = os.path.join(save_folder, f\"{event_id}.mseed\")\n",
    "                st.write(output_path, format=\"MSEED\")\n",
    "                print(f\"Saved waveform to {output_path}\")\n",
    "                return\n",
    "            except Exception as inner_e:\n",
    "                print(f\"Station {sta['network']}.{sta['station']} failed: {inner_e}\")\n",
    "\n",
    "        print(f\"All station attempts failed for event {event_id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download waveform for {event_id}: {e}\")\n",
    "\n",
    "# Loop over each network in your list\n",
    "for j in range(len(networks)):\n",
    "    print(networks[j])\n",
    "\n",
    "    # Limit processing to specific networks of interest\n",
    "    if networks[j] in ['EAF', 'PARK', 'UTA', 'NAF', 'HAY', 'CHAF', 'HOL', 'DSF', 'SOCAL', 'RID', 'CAL']:\n",
    "\n",
    "        # Filter creepmeters to just those in this network\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network'] != networks[j]].index)\n",
    "\n",
    "        # Loop over each creepmeter in the network\n",
    "        for i in range(len(creepmeters_network)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]\n",
    "            print(abbrv)\n",
    "\n",
    "            # Filter picks (creep events) for this creepmeter\n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation'] != abbrv].index)\n",
    "\n",
    "            # Skip if no picks for this creepmeter\n",
    "            if picks.empty:\n",
    "                continue\n",
    "\n",
    "            # Get creepmeter location\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv, lat, long)\n",
    "\n",
    "            # Extract per-event sampling interval (in seconds)\n",
    "            sampling_freq = picks['Sampling rate, s']\n",
    "\n",
    "            # Determine query time window from picks\n",
    "            starttime = UTCDateTime((pd.to_datetime(picks['Start Time']).min() - pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "            endtime = UTCDateTime((pd.to_datetime(picks['Start Time']).max() + pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "            # Query IRIS FDSN event service\n",
    "            client = Client(\"IRIS\")\n",
    "            try:\n",
    "                catalog = client.get_events(\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime,\n",
    "                    latitude=lat,\n",
    "                    longitude=long,\n",
    "                    maxradius=maxradius_deg,  # ~500 km\n",
    "                    minmagnitude=min_magnitude\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve events for {abbrv}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Parse earthquake metadata into DataFrame\n",
    "            eq_data = []\n",
    "            for event in catalog:\n",
    "                origin = event.preferred_origin() or event.origins[0]\n",
    "                magnitude = event.preferred_magnitude() or event.magnitudes[0]\n",
    "                eq_data.append({\n",
    "                    'eq_time': origin.time.datetime,\n",
    "                    'magnitude': magnitude.mag,\n",
    "                    'latitude': origin.latitude,\n",
    "                    'longitude': origin.longitude,\n",
    "                    'depth_km': origin.depth / 1000.0,\n",
    "                    'place': event.resource_id.id.split('/')[-1]\n",
    "                })\n",
    "\n",
    "            eq_df = pd.DataFrame(eq_data)\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            eq_df['eq_time'] = pd.to_datetime(eq_df['eq_time'])\n",
    "\n",
    "            # Search for plausible EQ triggers based on wave arrival time\n",
    "            expanded_matches = []\n",
    "            for idx, pick in picks.iterrows():\n",
    "                start_time = pick['Start Time']\n",
    "                sampling_interval = pick['Sampling rate, s']\n",
    "                pick_event_id = pick['Event_id']\n",
    "\n",
    "                valid_trigger_found = False\n",
    "                best_match = None\n",
    "                best_time_diff = np.inf\n",
    "\n",
    "                for _, eq in eq_df.iterrows():\n",
    "                    dist_km = geodesic((lat, long), (eq['latitude'], eq['longitude'])).km\n",
    "                    travel_time = dist_km / 3.5\n",
    "                    time_since_eq = (start_time - eq['eq_time']).total_seconds()\n",
    "                    upper_bound = travel_time + 2 * sampling_interval + 1.0 * sampling_interval + 60\n",
    "\n",
    "                    if travel_time <= time_since_eq <= upper_bound:\n",
    "                        if time_since_eq < best_time_diff:\n",
    "                            valid_trigger_found = True\n",
    "                            best_time_diff = time_since_eq\n",
    "                            best_match = {\n",
    "                                'Start Time': start_time,\n",
    "                                'abbrv': abbrv,\n",
    "                                'eq_time': eq['eq_time'],\n",
    "                                'magnitude': eq['magnitude'],\n",
    "                                'latitude': eq['latitude'],\n",
    "                                'longitude': eq['longitude'],\n",
    "                                'depth_km': eq['depth_km'],\n",
    "                                'place': eq['place'],\n",
    "                                'Time Since EQ [s]': time_since_eq,\n",
    "                                'Surface Wave Travel Time [s]': travel_time,\n",
    "                                'Upper Tolerance Bound [s]': upper_bound,\n",
    "                                'Likely Triggered': True,\n",
    "                                'Sampling Interval [s]': sampling_interval,\n",
    "                                'Latitude_creep': lat,\n",
    "                                'Longitude_creep': long,\n",
    "                                'event_id_from_picks': pick_event_id\n",
    "                            }\n",
    "\n",
    "                if not valid_trigger_found:\n",
    "                    best_match = {\n",
    "                        'Start Time': start_time,\n",
    "                        'abbrv': abbrv,\n",
    "                        'eq_time': pd.NaT,\n",
    "                        'magnitude': np.nan,\n",
    "                        'latitude': np.nan,\n",
    "                        'longitude': np.nan,\n",
    "                        'depth_km': np.nan,\n",
    "                        'place': None,\n",
    "                        'Time Since EQ [s]': np.nan,\n",
    "                        'Surface Wave Travel Time [s]': np.nan,\n",
    "                        'Upper Tolerance Bound [s]': np.nan,\n",
    "                        'Likely Triggered': False,\n",
    "                        'Sampling Interval [s]': sampling_interval,\n",
    "                        'Latitude_creep': lat,\n",
    "                        'Longitude_creep': long,\n",
    "                        'event_id_from_picks': pick_event_id\n",
    "                    }\n",
    "\n",
    "                expanded_matches.append(best_match)\n",
    "\n",
    "                # If valid, download waveform\n",
    "                if best_match['Likely Triggered']:\n",
    "                    event_id = best_match['event_id_from_picks']\n",
    "                    eq_lat = best_match['latitude']\n",
    "                    eq_lon = best_match['longitude']\n",
    "                    eq_time = best_match['eq_time']\n",
    "                    download_waveform_for_event(eq_time, eq_lat, eq_lon, event_id, lat, long)\n",
    "\n",
    "            nearest_eq_df = pd.DataFrame(expanded_matches)\n",
    "\n",
    "            # Compute percentage of events considered valid triggers\n",
    "            count_valid = nearest_eq_df['Likely Triggered'].sum()\n",
    "            percent_valid = 100 * count_valid / len(picks)\n",
    "\n",
    "            print(f\"{percent_valid:.2f}% of {len(picks)} events have valid earthquake triggers (wave arrival + resolution buffer).\")\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({\n",
    "                'Creepmeter': [abbrv],\n",
    "                'percentage valid triggers': [round(percent_valid, 2)],\n",
    "                'Latitude': lat\n",
    "            })\n",
    "\n",
    "            creepmeter_eq_picks = pd.concat([picks.reset_index(drop=True), nearest_eq_df.reset_index(drop=True)], axis=1)\n",
    "            my_data = pd.concat([my_data, creepmeter_eq_picks])\n",
    "            percentage_triggered_all = pd.concat([percentage_triggered_all, percentage_triggered])\n",
    "\n",
    "        percentage_triggered_all.sort_values(by='Latitude', inplace=True, ascending=False)\n",
    "        percentage_triggered_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "my_data.to_csv(\"triggered_creep_events_matched_to_nearest_EQ.csv\", index=False)\n",
    "percentage_triggered_all.to_csv(\"triggered_creep_percentage_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1acebac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAF\n",
      "PARK\n",
      "C461\n",
      "C461 35.724 -120.282\n",
      "0.00% of 43 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "C462\n",
      "C462 35.724 -120.282\n",
      "0.00% of 24 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "CRR1\n",
      "CRR1 35.835 -120.363\n",
      "0.00% of 139 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "WKR1\n",
      "WKR1 35.858 -120.392\n",
      "Trying station YH.CRAK (16.0 km) for all BH? channels\n",
      "Saved waveform to waveforms/2503_WKR1.mseed\n",
      "0.63% of 158 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "X461\n",
      "X461 35.723 -120.278\n",
      "Trying station BK.PKD (34.3 km) for all BH? channels\n",
      "Saved waveform to waveforms/2073_X461.mseed\n",
      "Trying station BK.PKD (34.3 km) for all BH? channels\n",
      "Saved waveform to waveforms/2075_X461.mseed\n",
      "1.37% of 146 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XGH1\n",
      "XGH1 35.82 -120.348\n",
      "0.00% of 88 events have valid earthquake triggers (wave arrival + resolution buffer).\n",
      "XHSW\n",
      "XHSW 35.862 -120.42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 133\u001b[0m\n\u001b[1;32m    123\u001b[0m global_catalog \u001b[38;5;241m=\u001b[39m global_client\u001b[38;5;241m.\u001b[39mget_events(\n\u001b[1;32m    124\u001b[0m     starttime\u001b[38;5;241m=\u001b[39mstarttime,\n\u001b[1;32m    125\u001b[0m     endtime\u001b[38;5;241m=\u001b[39mendtime,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     minmagnitude\u001b[38;5;241m=\u001b[39mmin_magnitude\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Query regional catalog\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m regional_catalog \u001b[38;5;241m=\u001b[39m \u001b[43mregional_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarttime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxradius_deg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminmagnitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_magnitude\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Combine events from both catalogs\u001b[39;00m\n\u001b[1;32m    143\u001b[0m combined_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(global_catalog) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(regional_catalog)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:556\u001b[0m, in \u001b[0;36mClient.get_events\u001b[0;34m(self, starttime, endtime, minlatitude, maxlatitude, minlongitude, maxlongitude, latitude, longitude, minradius, maxradius, mindepth, maxdepth, minmagnitude, maxmagnitude, magnitudetype, eventtype, includeallorigins, includeallmagnitudes, includearrivals, eventid, limit, offset, orderby, catalog, contributor, updatedafter, filename, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     data_stream\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[43mobspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquakeml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     data_stream\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cat\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/util/decorator.py:297\u001b[0m, in \u001b[0;36mmap_example_filename.<locals>._map_example_filename\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/event/catalog.py:810\u001b[0m, in \u001b[0;36mread_events\u001b[0;34m(pathname_or_url, format, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_example_catalog()\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generic_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathname_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/util/base.py:624\u001b[0m, in \u001b[0;36m_generic_reader\u001b[0;34m(pathname_or_url, callback_func, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pathname_or_url, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;66;03m# not a string - we assume a file-like object\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;66;03m# first try reading directly\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m         generic \u001b[38;5;241m=\u001b[39m \u001b[43mcallback_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathname_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;66;03m# if this fails, create a temporary file which is read directly\u001b[39;00m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;66;03m# from the file system\u001b[39;00m\n\u001b[1;32m    628\u001b[0m         pathname_or_url\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/util/decorator.py:142\u001b[0m, in \u001b[0;36muncompress_file\u001b[0;34m(func, filename, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(filename, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(filename)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    144\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/event/catalog.py:818\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filename, format, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;129m@uncompress_file\u001b[39m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read\u001b[39m(filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    815\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m    Reads a single event file into a ObsPy Catalog object.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m     catalog, \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_read_from_plugin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m catalog:\n\u001b[1;32m    821\u001b[0m         event\u001b[38;5;241m.\u001b[39m_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/core/util/base.py:423\u001b[0m, in \u001b[0;36m_read_from_plugin\u001b[0;34m(plugin_type, filename, format, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg \u001b[38;5;241m%\u001b[39m (format_ep\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(eps)))\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# read\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m list_obj \u001b[38;5;241m=\u001b[39m \u001b[43mread_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m list_obj, format_ep\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:1833\u001b[0m, in \u001b[0;36m_read_quakeml\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_quakeml\u001b[39m(filename):\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m    Reads a QuakeML file and returns an ObsPy Catalog object.\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;124;03m    2006-09-10T04:26:33.610000Z |  +9.614, +121.961 | 9.8  MS\u001b[39;00m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnpickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:149\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mReads QuakeML file into ObsPy catalog object.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m:returns: ObsPy Catalog object.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxml_doc \u001b[38;5;241m=\u001b[39m _xml_doc_from_anything(file)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:967\u001b[0m, in \u001b[0;36mUnpickler._deserialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m     arrival \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrival(arrival_el)\n\u001b[1;32m    965\u001b[0m     arrivals\u001b[38;5;241m.\u001b[39mappend(arrival)\n\u001b[0;32m--> 967\u001b[0m origin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_origin\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin_el\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrivals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrivals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# append origin with arrivals\u001b[39;00m\n\u001b[1;32m    970\u001b[0m event\u001b[38;5;241m.\u001b[39morigins\u001b[38;5;241m.\u001b[39mappend(origin)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:586\u001b[0m, in \u001b[0;36mUnpickler._origin\u001b[0;34m(self, element, arrivals)\u001b[0m\n\u001b[1;32m    584\u001b[0m obj\u001b[38;5;241m.\u001b[39mcreation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_creation_info(element)\n\u001b[1;32m    585\u001b[0m obj\u001b[38;5;241m.\u001b[39mcomments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comments(element)\n\u001b[0;32m--> 586\u001b[0m obj\u001b[38;5;241m.\u001b[39morigin_uncertainty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_origin_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m obj\u001b[38;5;241m.\u001b[39marrivals \u001b[38;5;241m=\u001b[39m arrivals\n\u001b[1;32m    588\u001b[0m obj\u001b[38;5;241m.\u001b[39mresource_id \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublicID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:398\u001b[0m, in \u001b[0;36mUnpickler._origin_uncertainty\u001b[0;34m(self, parent)\u001b[0m\n\u001b[1;32m    394\u001b[0m obj\u001b[38;5;241m.\u001b[39mmax_horizontal_uncertainty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xpath2obj(\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxHorizontalUncertainty\u001b[39m\u001b[38;5;124m'\u001b[39m, element, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m    396\u001b[0m obj\u001b[38;5;241m.\u001b[39mazimuth_max_horizontal_uncertainty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xpath2obj(\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazimuthMaxHorizontalUncertainty\u001b[39m\u001b[38;5;124m'\u001b[39m, element, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m--> 398\u001b[0m obj\u001b[38;5;241m.\u001b[39mconfidence_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xpath2obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfidenceLevel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m ce_el \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidenceEllipsoid\u001b[39m\u001b[38;5;124m'\u001b[39m, element)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:164\u001b[0m, in \u001b[0;36mUnpickler._xpath2obj\u001b[0;34m(self, xpath, element, convert_to, namespace)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_xpath2obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, xpath, element\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, convert_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 164\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m q:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/creepmeters/lib/python3.10/site-packages/obspy/io/quakeml/core.py:210\u001b[0m, in \u001b[0;36mUnpickler._xpath\u001b[0;34m(self, xpath, element, namespace)\u001b[0m\n\u001b[1;32m    207\u001b[0m     xpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m xpath\n\u001b[1;32m    208\u001b[0m     namespaces \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsmap[\u001b[38;5;28;01mNone\u001b[39;00m]}\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespaces\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "radius_km = 500\n",
    "maxradius_deg = radius_km / 111.0\n",
    "min_magnitude = 4  # Set minimum magnitude for EQ search\n",
    "\n",
    "# Channels to try\n",
    "channels_to_try = [\"BHZ\", \"BHE\", \"BHN\"]\n",
    "\n",
    "# Containers to store all output across networks\n",
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all = pd.DataFrame()\n",
    "\n",
    "# Function to download waveform from closest broadband station with fallback\n",
    "def download_waveform_for_event(event_time, eq_lat, eq_lon, event_id, creep_lat, creep_lon,\n",
    "                                 duration=300, save_folder=\"waveforms\", radius_deg=1.0):\n",
    "    client = Client(\"IRIS\")\n",
    "    starttime = UTCDateTime(event_time)\n",
    "    endtime = starttime + duration\n",
    "\n",
    "    try:\n",
    "        inventory = client.get_stations(\n",
    "            latitude=creep_lat,\n",
    "            longitude=creep_lon,\n",
    "            maxradius=radius_deg,\n",
    "            channel=\"BH?\",\n",
    "            level=\"channel\",\n",
    "            starttime=starttime,\n",
    "            endtime=endtime\n",
    "        )\n",
    "\n",
    "        if not inventory or len(inventory) == 0:\n",
    "            print(f\"No stations found near {event_id}\")\n",
    "            return\n",
    "\n",
    "        # Create list of candidate stations sorted by distance\n",
    "        station_list = []\n",
    "        for net in inventory:\n",
    "            for sta in net:\n",
    "                sta_lat = sta.latitude\n",
    "                sta_lon = sta.longitude\n",
    "                dist_km = geodesic((creep_lat, creep_lon), (sta_lat, sta_lon)).km\n",
    "                station_list.append({\n",
    "                    \"distance\": dist_km,\n",
    "                    \"network\": net.code,\n",
    "                    \"station\": sta.code,\n",
    "                    \"location\": \"*\"\n",
    "                })\n",
    "\n",
    "        station_list.sort(key=lambda x: x[\"distance\"])\n",
    "\n",
    "        # Try stations in order of proximity and download all BH? channels from the first working station\n",
    "        for sta in station_list:\n",
    "            try:\n",
    "                print(f\"Trying station {sta['network']}.{sta['station']} ({sta['distance']:.1f} km) for all BH? channels\")\n",
    "                st = client.get_waveforms(\n",
    "                    network=sta[\"network\"],\n",
    "                    station=sta[\"station\"],\n",
    "                    location=sta[\"location\"],\n",
    "                    channel=\"BH?\",\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime\n",
    "                )\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                output_path = os.path.join(save_folder, f\"{event_id}.mseed\")\n",
    "                st.write(output_path, format=\"MSEED\")\n",
    "                print(f\"Saved waveform to {output_path}\")\n",
    "                return\n",
    "            except Exception as inner_e:\n",
    "                print(f\"Station {sta['network']}.{sta['station']} failed: {inner_e}\")\n",
    "\n",
    "        print(f\"All station attempts failed for event {event_id}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download waveform for {event_id}: {e}\")\n",
    "\n",
    "# Define FDSN clients for global and regional catalogs\n",
    "global_client = Client(\"IRIS\")\n",
    "regional_client = Client(\"NCEDC\")  # Northern California Earthquake Data Center\n",
    "\n",
    "# Loop over each network in your list\n",
    "for j in range(len(networks)):\n",
    "    print(networks[j])\n",
    "\n",
    "    # Limit processing to specific networks of interest\n",
    "    if networks[j] in ['HOL','HAY','SOCAL','PARK']:#['EAF', 'PARK', 'UTA', 'NAF', 'HAY', 'CHAF', 'HOL', 'DSF', 'SOCAL', 'RID', 'CAL']:\n",
    "\n",
    "        # Filter creepmeters to just those in this network\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network'] != networks[j]].index)\n",
    "\n",
    "        # Loop over each creepmeter in the network\n",
    "        for i in range(len(creepmeters_network)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]\n",
    "            print(abbrv)\n",
    "\n",
    "            # Filter picks (creep events) for this creepmeter\n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation'] != abbrv].index)\n",
    "\n",
    "            # Skip if no picks for this creepmeter\n",
    "            if picks.empty:\n",
    "                continue\n",
    "\n",
    "            # Get creepmeter location\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv, lat, long)\n",
    "\n",
    "            # Extract per-event sampling interval (in seconds)\n",
    "            sampling_freq = picks['Sampling rate, s']\n",
    "\n",
    "            # Determine query time window from picks\n",
    "            starttime = UTCDateTime((pd.to_datetime(picks['Start Time']).min() - pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "            endtime = UTCDateTime((pd.to_datetime(picks['Start Time']).max() + pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "\n",
    "            try:\n",
    "                # Query global catalog\n",
    "                global_catalog = global_client.get_events(\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime,\n",
    "                    latitude=lat,\n",
    "                    longitude=long,\n",
    "                    maxradius=maxradius_deg,  # ~500 km\n",
    "                    minmagnitude=min_magnitude\n",
    "                )\n",
    "\n",
    "                # Query regional catalog\n",
    "                regional_catalog = regional_client.get_events(\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime,\n",
    "                    latitude=lat,\n",
    "                    longitude=long,\n",
    "                    maxradius=maxradius_deg,\n",
    "                    minmagnitude=min_magnitude\n",
    "                )\n",
    "\n",
    "                # Combine events from both catalogs\n",
    "                combined_events = list(global_catalog) + list(regional_catalog)\n",
    "\n",
    "                # Deduplicate events by origin time and rounded lat/lon (4 decimals)\n",
    "                unique_events_dict = {}\n",
    "                for event in combined_events:\n",
    "                    origin = event.preferred_origin() or event.origins[0]\n",
    "                    key = (\n",
    "                        origin.time.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\n",
    "                        round(origin.latitude, 4),\n",
    "                        round(origin.longitude, 4)\n",
    "                    )\n",
    "                    unique_events_dict[key] = event\n",
    "\n",
    "                combined_catalog = list(unique_events_dict.values())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve events for {abbrv}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Parse earthquake metadata into DataFrame\n",
    "            eq_data = []\n",
    "            for event in combined_catalog:\n",
    "                origin = event.preferred_origin() or event.origins[0]\n",
    "                magnitude = event.preferred_magnitude() or event.magnitudes[0]\n",
    "                eq_data.append({\n",
    "                    'eq_time': origin.time.datetime,\n",
    "                    'magnitude': magnitude.mag,\n",
    "                    'latitude': origin.latitude,\n",
    "                    'longitude': origin.longitude,\n",
    "                    'depth_km': origin.depth / 1000.0,\n",
    "                    'place': event.resource_id.id.split('/')[-1]\n",
    "                })\n",
    "\n",
    "            eq_df = pd.DataFrame(eq_data)\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            eq_df['eq_time'] = pd.to_datetime(eq_df['eq_time'])\n",
    "\n",
    "            # Search for plausible EQ triggers based on wave arrival time\n",
    "            expanded_matches = []\n",
    "            for idx, pick in picks.iterrows():\n",
    "                start_time = pick['Start Time']\n",
    "                sampling_interval = pick['Sampling rate, s']\n",
    "                pick_event_id = pick['Event_id']\n",
    "\n",
    "                valid_trigger_found = False\n",
    "                best_match = None\n",
    "                best_time_diff = np.inf\n",
    "\n",
    "                for _, eq in eq_df.iterrows():\n",
    "                    dist_km = geodesic((lat, long), (eq['latitude'], eq['longitude'])).km\n",
    "                    travel_time = dist_km / 3.5\n",
    "                    time_since_eq = (start_time - eq['eq_time']).total_seconds()\n",
    "                    upper_bound = travel_time + 2 * sampling_interval + 1.0 * sampling_interval + 60\n",
    "\n",
    "                    if travel_time <= time_since_eq <= upper_bound:\n",
    "                        if time_since_eq < best_time_diff:\n",
    "                            valid_trigger_found = True\n",
    "                            best_time_diff = time_since_eq\n",
    "                            best_match = {\n",
    "                                'Start Time': start_time,\n",
    "                                'abbrv': abbrv,\n",
    "                                'eq_time': eq['eq_time'],\n",
    "                                'magnitude': eq['magnitude'],\n",
    "                                'latitude': eq['latitude'],\n",
    "                                'longitude': eq['longitude'],\n",
    "                                'depth_km': eq['depth_km'],\n",
    "                                'place': eq['place'],\n",
    "                                'Time Since EQ [s]': time_since_eq,\n",
    "                                'Surface Wave Travel Time [s]': travel_time,\n",
    "                                'Upper Tolerance Bound [s]': upper_bound,\n",
    "                                'Likely Triggered': True,\n",
    "                                'Sampling Interval [s]': sampling_interval,\n",
    "                                'Latitude_creep': lat,\n",
    "                                'Longitude_creep': long,\n",
    "                                'event_id_from_picks': pick_event_id\n",
    "                            }\n",
    "\n",
    "                if not valid_trigger_found:\n",
    "                    best_match = {\n",
    "                        'Start Time': start_time,\n",
    "                        'abbrv': abbrv,\n",
    "                        'eq_time': pd.NaT,\n",
    "                        'magnitude': np.nan,\n",
    "                        'latitude': np.nan,\n",
    "                        'longitude': np.nan,\n",
    "                        'depth_km': np.nan,\n",
    "                        'place': None,\n",
    "                        'Time Since EQ [s]': np.nan,\n",
    "                        'Surface Wave Travel Time [s]': np.nan,\n",
    "                        'Upper Tolerance Bound [s]': np.nan,\n",
    "                        'Likely Triggered': False,\n",
    "                        'Sampling Interval [s]': sampling_interval,\n",
    "                        'Latitude_creep': lat,\n",
    "                        'Longitude_creep': long,\n",
    "                        'event_id_from_picks': pick_event_id\n",
    "                    }\n",
    "\n",
    "                expanded_matches.append(best_match)\n",
    "\n",
    "                # If valid, download waveform\n",
    "                if best_match['Likely Triggered']:\n",
    "                    event_id = best_match['event_id_from_picks']\n",
    "                    eq_lat = best_match['latitude']\n",
    "                    eq_lon = best_match['longitude']\n",
    "                    eq_time = best_match['eq_time']\n",
    "                    download_waveform_for_event(eq_time, eq_lat, eq_lon, event_id, lat, long)\n",
    "\n",
    "            nearest_eq_df = pd.DataFrame(expanded_matches)\n",
    "\n",
    "            # Compute percentage of events considered valid triggers\n",
    "            count_valid = nearest_eq_df['Likely Triggered'].sum()\n",
    "            percent_valid = 100 * count_valid / len(picks)\n",
    "\n",
    "            print(f\"{percent_valid:.2f}% of {len(picks)} events have valid earthquake triggers (wave arrival + resolution buffer).\")\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({\n",
    "                'Creepmeter': [abbrv],\n",
    "                'percentage valid triggers': [round(percent_valid, 2)],\n",
    "                'Latitude': lat\n",
    "            })\n",
    "\n",
    "            creepmeter_eq_picks = pd.concat([picks.reset_index(drop=True), nearest_eq_df.reset_index(drop=True)], axis=1)\n",
    "            my_data = pd.concat([my_data, creepmeter_eq_picks])\n",
    "            percentage_triggered_all = pd.concat([percentage_triggered_all, percentage_triggered])\n",
    "\n",
    "        percentage_triggered_all.sort_values(by='Latitude', inplace=True, ascending=False)\n",
    "        percentage_triggered_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Save outputs\n",
    "my_data.to_csv(\"triggered_creep_events_matched_to_nearest_EQ.csv\", index=False)\n",
    "percentage_triggered_all.to_csv(\"triggered_creep_percentage_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c696a",
   "metadata": {},
   "source": [
    "Other things for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab0427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all_rainfall = pd.DataFrame()\n",
    "for j in range(len(networks)):\n",
    "    print(networks[j])\n",
    "    if networks[j] in ['HOL']:\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network']!=networks[j]].index)\n",
    "        for i in range(len(creepmeters_network)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]            \n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation']!=abbrv].index)\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv,lat,long)\n",
    "            rainfall = pd.read_csv('../../Data/ECMWF/Creepmeter_precipitation/{p}/Precipitation_ECMWF_{q}_02_JUL_2024.csv'.format(p=networks[j],q=abbrv))\n",
    "\n",
    "            # Ensure datetime format and sort\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            rainfall['rain_time'] = pd.to_datetime(rainfall['Time (UTC)'])\n",
    "\n",
    "            picks = picks.sort_values('Start Time').reset_index(drop=True)\n",
    "            rainfall = rainfall.sort_values('rain_time').reset_index(drop=True)\n",
    "\n",
    "            # Set rain_time as index for fast slicing\n",
    "            rainfall = rainfall.set_index('Time (UTC)')\n",
    "\n",
    "            # Ensure the index is sorted\n",
    "            rainfall = rainfall.sort_index()\n",
    "\n",
    "            def rain_in_previous_hour(pick_time):\n",
    "                pick_time = pd.to_datetime(pick_time)\n",
    "                start = pick_time - pd.Timedelta(hours=1)\n",
    "                # Clip start and end to be within the rainfall index range\n",
    "                rainfall.index = pd.to_datetime(rainfall.index)\n",
    "                start = max(start, rainfall.index.min())\n",
    "                end = min(pick_time, rainfall.index.max())\n",
    "\n",
    "                rain_window = rainfall.loc[start:end]\n",
    "                if rain_window.empty:\n",
    "                    return 0.0\n",
    "                else:\n",
    "                    return rain_window['Precipitation, m'].sum()\n",
    "\n",
    "            picks['rain_last_hour'] = picks['Start Time'].apply(rain_in_previous_hour)\n",
    "            picks['rain_before'] = picks['rain_last_hour'] > 0.0005\n",
    "            \n",
    "            if abbrv in ['XHR2']:\n",
    "                boolarr = picks['rain_before']==True\n",
    "                print(picks[boolarr])\n",
    "\n",
    "\n",
    "            # Count how many are within 1 day\n",
    "            count_within_hour = picks['rain_before'].sum()\n",
    "    \n",
    "\n",
    "            # Total number of entries\n",
    "            total_count = len(picks)\n",
    "\n",
    "            # Percentage\n",
    "            percent_within_hour = 100 * count_within_hour / total_count\n",
    "\n",
    "            #print(f\"{percent_within_day:.2f}% of {total_count} events occur within 1 day of an earthquake.\")\n",
    "            print(f\"{percent_within_hour:.2f}% of {total_count} events occur within 1 hour of 0.5mm of rainfall.\")\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({'Creepmeter':[abbrv],'percentage in hour':[round(percent_within_hour,2)],\n",
    "                                                    'Latitude':lat})\n",
    "\n",
    "\n",
    "            percentage_triggered_all_rainfall = pd.concat([percentage_triggered_all_rainfall,percentage_triggered])\n",
    "        percentage_triggered_all_rainfall.sort_values(by='Latitude',inplace=True,ascending=False)\n",
    "        percentage_triggered_all_rainfall.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=cmc.managua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours  = cmap(np.linspace(0, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3191b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(1,1,1)\n",
    "#plt.plot(percentage_triggered_all['Latitude'],percentage_triggered_all['percentage in day'])\n",
    "# Add a rectangle: (x, y) = lower left corner; width and height\n",
    "rect_Hawyard = patches.Rectangle((-0.5, 0), width=3, height=60, linewidth=2, edgecolor=colours[0], facecolor=colours[0],alpha=0.2)\n",
    "rect_Calaveras = patches.Rectangle((2.5, 0), width=1, height=60, linewidth=2, edgecolor=colours[1], facecolor=colours[1],alpha=0.2)\n",
    "rect_Hollister = patches.Rectangle((3.5, 0), width=10, height=60, linewidth=2, edgecolor=colours[2], facecolor=colours[2],alpha=0.2)\n",
    "rect_Parkfield = patches.Rectangle((13.5, 0), width=15, height=60, linewidth=2, edgecolor=colours[4], facecolor=colours[4],alpha=0.2)\n",
    "rect_SoCal = patches.Rectangle((28.5, 0), width=10, height=60, linewidth=2, edgecolor=colours[6], facecolor=colours[6],alpha=0.2)\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(percentage_triggered_all)),percentage_triggered_all['Creepmeter'],rotation=90)\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect_Hawyard)\n",
    "ax.add_patch(rect_Calaveras)\n",
    "ax.add_patch(rect_Hollister)\n",
    "ax.add_patch(rect_Parkfield)\n",
    "ax.add_patch(rect_SoCal)\n",
    "\n",
    "plt.plot(np.arange(len(percentage_triggered_all)),percentage_triggered_all['percentage in hour'],\n",
    "         color=colours[3],marker='o',label='Earthquake in hour prior')\n",
    "plt.plot(np.arange(len(percentage_triggered_all_rainfall)),percentage_triggered_all_rainfall['percentage in hour'],\n",
    "         color=colours[5],marker='^',label='Rainfall in hour prior')\n",
    "plt.ylabel(\"% of events\")\n",
    "plt.xlabel('Creepmeter')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af8728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a5fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all_rainfall_boot = pd.DataFrame()\n",
    "for j in range(len(networks)):\n",
    "    print(networks[j])\n",
    "    if networks[j] in ['SOCAL','HAY','HOL','CAL','PARK']:\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network']!=networks[j]].index)\n",
    "        for i in range(len(creepmeters_network)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]            \n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation']!=abbrv].index)\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv,lat,long)\n",
    "            rainfall = pd.read_csv('../../Data/ECMWF/Creepmeter_precipitation/{p}/Precipitation_ECMWF_{q}_02_JUL_2024.csv'.format(p=networks[j],q=abbrv))\n",
    "\n",
    "            # Ensure datetime format and sort\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            rainfall['rain_time'] = pd.to_datetime(rainfall['Time (UTC)'])\n",
    "\n",
    "            picks = picks.sort_values('Start Time').reset_index(drop=True)\n",
    "            rainfall = rainfall.sort_values('rain_time').reset_index(drop=True)\n",
    "\n",
    "            # Set rain_time as index for fast slicing\n",
    "            rainfall = rainfall.set_index('Time (UTC)')\n",
    "\n",
    "            # Ensure the index is sorted\n",
    "            rainfall = rainfall.sort_index()\n",
    "\n",
    "            def rain_in_previous_hour(pick_time):\n",
    "                pick_time = pd.to_datetime(pick_time)\n",
    "                start = pick_time - pd.Timedelta(hours=1)\n",
    "                #start = pick_time - pd.Timedelta(days = 1)\n",
    "                # Clip start and end to be within the rainfall index range\n",
    "                rainfall.index = pd.to_datetime(rainfall.index)\n",
    "                start = max(start, rainfall.index.min())\n",
    "                end = min(pick_time, rainfall.index.max())\n",
    "\n",
    "                rain_window = rainfall.loc[start:end]\n",
    "                if rain_window.empty:\n",
    "                    return 0.0\n",
    "                else:\n",
    "                    return rain_window['Precipitation, m'].sum()\n",
    "\n",
    "            picks['rain_last_hour'] = picks['Start Time'].apply(rain_in_previous_hour)\n",
    "            picks['rain_before'] = picks['rain_last_hour'] > 0.0005\n",
    "            \n",
    "            # Total number of entries\n",
    "            total_count = len(picks)\n",
    "            \n",
    "            bootstrapped = []\n",
    "            for m in tqdm(range(1000)):\n",
    "                selected = []\n",
    "                for n in range(len(picks)):\n",
    "                    rand_index = random.randint(0, len(picks) - 1)\n",
    "                    selected.append(picks['rain_before'].iloc[rand_index])\n",
    "                \n",
    "                # Count how many are within 1 day\n",
    "                count_within_hour = np.sum(selected)            \n",
    "\n",
    "                # Percentage\n",
    "                percent_within_hour = 100 * (count_within_hour / len(picks))\n",
    "                bootstrapped.append(percent_within_hour)\n",
    "\n",
    "\n",
    "            per_85 = np.percentile(bootstrapped,85)\n",
    "            per_15 = np.percentile(bootstrapped,15)\n",
    "            median = np.median(bootstrapped)\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({'Creepmeter':[abbrv],'per_85':[per_85],'per_15':[per_15],'median':[median],'Latitude':lat})\n",
    "\n",
    "\n",
    "            percentage_triggered_all_rainfall_boot = pd.concat([percentage_triggered_all_rainfall_boot,percentage_triggered])\n",
    "        percentage_triggered_all_rainfall_boot.sort_values(by='Latitude',inplace=True,ascending=False)\n",
    "        percentage_triggered_all_rainfall_boot.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_triggered_all_rainfall_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cee9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_within_hour/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fba142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fdc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame()\n",
    "percentage_triggered_all_eq_boot = pd.DataFrame()\n",
    "for j in tqdm(range(len(networks))):\n",
    "    print(networks[j])\n",
    "    if networks[j] in ['PARK','SOCAL','HAY','HOL','CAL']:\n",
    "        creepmeters_network = creepmeters.drop(creepmeters[creepmeters['Network']!=networks[j]].index)\n",
    "        for i in range(len(creepmeters_network)):#len(creepmeters)):\n",
    "            abbrv = creepmeters_network['Creepmeter_abbrv'].iloc[i]\n",
    "            print(abbrv)\n",
    "            \n",
    "            picks = creepmeters_picks.drop(creepmeters_picks[creepmeters_picks['Creepmeter abbreviation']!=abbrv].index)\n",
    "            lat = creepmeters_network['Latitude'].iloc[i]\n",
    "            long = creepmeters_network['Longitude'].iloc[i]\n",
    "            print(abbrv,lat,long)\n",
    "\n",
    "\n",
    "            # USGS API query parameters\n",
    "            url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "            params = {\n",
    "                \"format\": \"geojson\",\n",
    "                \"starttime\": \"1980-01-01\",\n",
    "                \"endtime\": \"2024-01-01\",\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": long,\n",
    "                \"maxradiuskm\": 500,\n",
    "                \"minmagnitude\": 4\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            # Extract earthquake metadata\n",
    "            eq_data = []\n",
    "            for feature in data['features']:\n",
    "                time = datetime.utcfromtimestamp(feature['properties']['time'] / 1000.0)\n",
    "                magnitude = feature['properties']['mag']\n",
    "                place = feature['properties']['place']\n",
    "                lon_eq, lat_eq, depth = feature['geometry']['coordinates']  # [lon, lat, depth]\n",
    "                eq_data.append({\n",
    "                    'eq_time': time,\n",
    "                    'magnitude': magnitude,\n",
    "                    'latitude': lat_eq,\n",
    "                    'longitude': lon_eq,\n",
    "                    'depth_km': depth,\n",
    "                    'place': place\n",
    "                })\n",
    "\n",
    "            # Create DataFrame\n",
    "            eq_df = pd.DataFrame(eq_data)\n",
    "\n",
    "\n",
    "            # Ensure both time columns are datetime\n",
    "            picks['Start Time'] = pd.to_datetime(picks['Start Time'])\n",
    "            eq_df['eq_time'] = pd.to_datetime(eq_df['eq_time'])\n",
    "\n",
    "            # Sort both by time (important!)\n",
    "            picks = picks.sort_values('Start Time').reset_index(drop=True)\n",
    "            eq_df = eq_df.sort_values('eq_time').reset_index(drop=True)\n",
    "\n",
    "            # Step 1: Use merge_asof twice â€” once backward and once forward\n",
    "            df_before = pd.merge_asof(picks, eq_df, left_on='Start Time', right_on='eq_time', direction='backward', suffixes=('', '_before'))\n",
    "            # Forward merge\n",
    "            df_after = pd.merge_asof(picks, eq_df, left_on='Start Time', right_on='eq_time', direction='forward')\n",
    "\n",
    "            # Rename the columns to prevent confusion\n",
    "            df_after = df_after.rename(columns={\n",
    "                'eq_time': 'eq_time_after',\n",
    "                'magnitude': 'magnitude_after',\n",
    "                'latitude': 'latitude_after',\n",
    "                'longitude': 'longitude_after',\n",
    "                'depth_km': 'depth_km_after',\n",
    "                'place': 'place_after'\n",
    "            })\n",
    "\n",
    "\n",
    "            # Step 2: Calculate time differences for each\n",
    "            df_before['dt_before'] = (df_before['Start Time'] - df_before['eq_time']).dt.total_seconds().abs()\n",
    "            df_after['dt_after'] = (df_after['Start Time'] - df_after['eq_time_after']).dt.total_seconds().abs()\n",
    "\n",
    "            print(\"Picks with no backward match:\", df_before['eq_time'].isna().sum())\n",
    "            print(\"Picks with no forward match:\", df_after['eq_time_after'].isna().sum())\n",
    "            print(len(picks))\n",
    "\n",
    "            # Step 3: Choose the closer one\n",
    "            nearest_eq = []\n",
    "            for i in range(len(picks)):\n",
    "                dt_b = df_before.loc[i, 'dt_before']\n",
    "                dt_a = df_after.loc[i, 'dt_after']\n",
    "                \n",
    "                # Use backward if forward is missing or farther\n",
    "                if pd.isna(dt_a) or (not pd.isna(dt_b) and dt_b <= dt_a):\n",
    "                    row = df_before.loc[i]\n",
    "                    nearest_eq.append({\n",
    "                        'Start Time': row['Start Time'],\n",
    "                        'abbrv': abbrv,\n",
    "                        'eq_time': row['eq_time'],\n",
    "                        'magnitude': row['magnitude'],\n",
    "                        'latitude': row['latitude'],\n",
    "                        'longitude': row['longitude'],\n",
    "                        'depth_km': row['depth_km'],\n",
    "                        'place': row['place'],\n",
    "                        'time_diff_seconds': row['dt_before']\n",
    "                    })\n",
    "                else:\n",
    "                    row = df_after.loc[i]\n",
    "                    nearest_eq.append({\n",
    "                        'Start Time': row['Start Time'],\n",
    "                        'abbrv': abbrv,\n",
    "                        'eq_time': row['eq_time_after'],\n",
    "                        'magnitude': row['magnitude_after'],\n",
    "                        'latitude': row['latitude_after'],\n",
    "                        'longitude': row['longitude_after'],\n",
    "                        'depth_km': row['depth_km_after'],\n",
    "                        'place': row['place_after'],\n",
    "                        'time_diff_seconds': row['dt_after']\n",
    "                    })\n",
    "\n",
    "            # Step 4: Convert to DataFrame\n",
    "            nearest_eq_df = pd.DataFrame(nearest_eq)\n",
    "\n",
    "            # Add time difference column\n",
    "            nearest_eq_df['time_since_eq_seconds'] = (nearest_eq_df['Start Time'] - nearest_eq_df['eq_time']).dt.total_seconds()\n",
    "\n",
    "            # Threshold in seconds (1 day)\n",
    "            one_day_seconds = 86400\n",
    "            one_hour_seconds = 3600\n",
    "\n",
    "            boolarr = abs(nearest_eq_df['time_since_eq_seconds']) < one_hour_seconds\n",
    "            #boolarr = np.logical_and(nearest_eq_df['time_since_eq_seconds']>-3600,nearest_eq_df['time_since_eq_seconds'] < one_day_seconds)\n",
    "\n",
    "            eq_trig = np.zeros(len(nearest_eq_df))\n",
    "            eq_trig[boolarr] = 1\n",
    "\n",
    "            nearest_eq_df['eq_trig'] = eq_trig\n",
    "            bootstrapped = []\n",
    "            for m in tqdm(range(1000)):\n",
    "                selected = []\n",
    "                for n in range(len(picks)):\n",
    "                    rand_index = random.randint(0, len(picks) - 1)\n",
    "                    selected.append(nearest_eq_df['eq_trig'].iloc[rand_index])\n",
    "                \n",
    "                # Count how many are within 1 day\n",
    "                count_within_hour = np.sum(selected)     \n",
    "                #print(count_within_hour,len(picks))       \n",
    "\n",
    "                # Percentage\n",
    "                percent_within_hour = 100 * (count_within_hour / len(picks))\n",
    "                #print(percent_within_hour)\n",
    "                bootstrapped.append(percent_within_hour)\n",
    "\n",
    "\n",
    "            per_85 = np.percentile(bootstrapped,85)\n",
    "            per_15 = np.percentile(bootstrapped,15)\n",
    "            median = np.median(bootstrapped)\n",
    "\n",
    "            percentage_triggered = pd.DataFrame({'Creepmeter':[abbrv],'per_85':[round(per_85,2)],'per_15':[per_15],'median':[median],'Latitude':lat})\n",
    "\n",
    "\n",
    "            percentage_triggered_all_eq_boot = pd.concat([percentage_triggered_all_eq_boot,percentage_triggered])\n",
    "        percentage_triggered_all_eq_boot.sort_values(by='Latitude',inplace=True,ascending=False)\n",
    "        percentage_triggered_all_eq_boot.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_triggered_all_eq_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a26fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "ax = plt.subplot(1,1,1)\n",
    "#plt.plot(percentage_triggered_all['Latitude'],percentage_triggered_all['percentage in day'])\n",
    "# Add a rectangle: (x, y) = lower left corner; width and height\n",
    "rect_Hawyard = patches.Rectangle((-0.5, -1), width=4, height=101, linewidth=2, edgecolor=colours[0], \n",
    "                                 facecolor=colours[0],alpha=0.2)\n",
    "rect_Calaveras = patches.Rectangle((3.5, -1), width=1, height=101, linewidth=2, edgecolor=colours[1], \n",
    "                                   facecolor=colours[1],alpha=0.2)\n",
    "rect_Hollister = patches.Rectangle((4.5, -1), width=10, height=101, linewidth=2, edgecolor=colours[2], \n",
    "                                   facecolor=colours[2],alpha=0.2)\n",
    "rect_Parkfield = patches.Rectangle((14.5, -1), width=15, height=101, linewidth=2, edgecolor=colours[4],\n",
    "                                    facecolor=colours[4],alpha=0.2)\n",
    "rect_SoCal = patches.Rectangle((29.5, -1), width=7, height=101, linewidth=2, edgecolor=colours[6], \n",
    "                               facecolor=colours[6],alpha=0.2)\n",
    "rect_NS = patches.Rectangle((36.5, -1), width=1, height=101, linewidth=2, edgecolor=colours[7], \n",
    "                            facecolor=colours[7],alpha=0.2)\n",
    "rect_SU = patches.Rectangle((37.5, -1), width=1, height=101, linewidth=2, edgecolor=colours[8], \n",
    "                            facecolor=colours[8],alpha=0.2)\n",
    "rect_IM = patches.Rectangle((38.5, -1), width=1, height=101, linewidth=2, edgecolor=colours[9], \n",
    "                            facecolor=colours[9],alpha=0.2)\n",
    "\n",
    "plt.xticks(np.arange(len(percentage_triggered_all_eq_boot)),percentage_triggered_all_eq_boot['Creepmeter'],rotation=90)\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect_Hawyard)\n",
    "ax.add_patch(rect_Calaveras)\n",
    "ax.add_patch(rect_Hollister)\n",
    "ax.add_patch(rect_Parkfield)\n",
    "ax.add_patch(rect_SoCal)\n",
    "ax.add_patch(rect_NS)\n",
    "ax.add_patch(rect_SU)\n",
    "ax.add_patch(rect_IM)\n",
    "plt.hlines(0,-0.5,39.5,colors='k')\n",
    "\n",
    "lower_err = percentage_triggered_all_eq_boot['median'] - percentage_triggered_all_eq_boot['per_15']\n",
    "upper_err = percentage_triggered_all_eq_boot['per_85'] - percentage_triggered_all_eq_boot['median']\n",
    "\n",
    "plt.errorbar(np.arange(len(percentage_triggered_all_eq_boot)), percentage_triggered_all_eq_boot['median'], \n",
    "             yerr=[lower_err, upper_err], \n",
    "             fmt='o', capsize=4,color=colours[3])\n",
    "\n",
    "plt.plot(np.arange(len(percentage_triggered_all_eq_boot)),percentage_triggered_all_eq_boot['median'],\n",
    "         color=colours[3],marker='o',label='Earthquake in hour prior')\n",
    "\n",
    "\n",
    "lower_err = percentage_triggered_all_rainfall_boot['median'] - percentage_triggered_all_rainfall_boot['per_15']\n",
    "upper_err = percentage_triggered_all_rainfall_boot['per_85'] - percentage_triggered_all_rainfall_boot['median']\n",
    "\n",
    "plt.errorbar(np.arange(len(percentage_triggered_all_rainfall_boot)), percentage_triggered_all_rainfall_boot['median'], \n",
    "             yerr=[lower_err, upper_err], \n",
    "             fmt='o', capsize=4,color=colours[5])\n",
    "\n",
    "plt.plot(np.arange(len(percentage_triggered_all_rainfall_boot)),percentage_triggered_all_rainfall_boot['median'],\n",
    "         color=colours[5],marker='o',label='Rainfall in hour prior')\n",
    "\n",
    "#plt.plot(np.arange(len(percentage_triggered_all_rainfall)),percentage_triggered_all_rainfall['percentage in hour'],\n",
    "#         color=colours[5],marker='^',label='Rainfall in hour prior')\n",
    "plt.ylabel(\"% of events\")\n",
    "plt.xlabel('Creepmeter')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.01, 0.82))\n",
    "plt.text(1.5,95,'Hayward Fault',verticalalignment='center',horizontalalignment='center')\n",
    "plt.text(4,95,'Calaveras\\nFault', rotation=90,verticalalignment='top',horizontalalignment='center')\n",
    "plt.text(9,95,'San Andreas Fault: Hollister',verticalalignment='center',horizontalalignment='center')\n",
    "plt.text(21.5,95,'San Andreas Fault: Parkfield',verticalalignment='center',horizontalalignment='center')\n",
    "plt.text(33,95,'San Andreas Fault: Salton Sea',verticalalignment='center',horizontalalignment='center')\n",
    "plt.text(37,95,'North Shoreline\\nFault', rotation=90,verticalalignment='top',horizontalalignment='center')\n",
    "plt.text(38,95,'Superstition Hills\\nFault', rotation=90,verticalalignment='top',horizontalalignment='center')\n",
    "plt.text(39,95,'Imperial\\nFault', rotation=90,verticalalignment='top',horizontalalignment='center')\n",
    "plt.xlim([-0.5,39.5])\n",
    "plt.ylim([-1,100])\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../Creep_catalog_stats_figures/Bootstrapped_percentages_events_rain_or_EQ_in_hour_prior.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks['Start Time']+dt.timedelta(days=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def add_one_year(date_value):\n",
    "  \"\"\"Adds one year to a date, handling leap years correctly.\"\"\"\n",
    "  new_date = date_value + relativedelta(years=1)\n",
    "  looping = int(str(picks['Start Time'].iloc[-1])[0:4]) - int(str(picks['Start Time'].iloc[0])[0:4])\n",
    "  if new_date > dt.datetime(2024,1,1):\n",
    "    new_date = new_date - relativedelta(years=looping+1)\n",
    "  return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a00dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks_test_circle = picks.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(str(picks_test_circle['Start Time'].iloc[-1])[0:4]) - int(str(picks_test_circle['Start Time'].iloc[0])[0:4])):\n",
    "    picks_test_circle['Start Time'] = picks_test_circle['Start Time'].apply(add_one_year)\n",
    "    print(picks_test_circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(str(picks['Start Time'].iloc[0])[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb492de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creepmeters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
