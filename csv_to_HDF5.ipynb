{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "'''import inflect\n",
    "c2h.stringify = inflect.engine()'''\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import csv_to_hdf5 as c2h\n",
    "import obspy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creepmeter_metadata = pd.read_excel('../../Data/Creepmeter_list.xlsx')\n",
    "creepmeter_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "directory = '../../Data/DATA_tidied/CSV/'  # Replace with your actual directory path\n",
    "files = c2h.list_files_in_directory(directory)\n",
    "print(files)\n",
    "try:\n",
    "    Creepmeter_dataframe = pd.read_csv('../../Data/DATA_tidied/creepmeter_metadata_post_standardisation.csv',index_col=0)\n",
    "except FileNotFoundError:\n",
    "    Creepmeter_dataframe = pd.DataFrame()\n",
    "\n",
    "print(Creepmeter_dataframe)\n",
    "Creepmeter_dataframe_SAC = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del files[6]\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['bal1.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.sort()\n",
    "diff_all = []\n",
    "count = 0\n",
    "for file in tqdm(files):\n",
    "    data = pd.read_csv(directory + file,index_col=0)\n",
    "    try:\n",
    "        #print(data['Time'].head(10))\n",
    "        data['Time'] = pd.to_datetime(data['Time'])\n",
    "    except KeyError:\n",
    "        #print(data['Date'].head(10))\n",
    "        data['Time'] = pd.to_datetime(data['Date'])\n",
    "    tm_diff = np.diff(data['Time'])/ np.timedelta64(1, 'm')\n",
    "    negatives = np.any(tm_diff<0)\n",
    "    if negatives == True:\n",
    "        where_neg = np.where(tm_diff<0)[0]\n",
    "        count+=1\n",
    "        print(where_neg)\n",
    "        for i in range(len(where_neg)):\n",
    "            print(data.loc[where_neg[i]-10:where_neg[i]+10])\n",
    "        print(file, 'negatives!!')\n",
    "    diff_all.extend(np.unique(tm_diff))\n",
    "print(count)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAL1\n",
      "../../Data/DATA_tidied/HDF5/BAL1.h5\n",
      "C461\n",
      "../../Data/DATA_tidied/HDF5/C461.h5\n",
      "C462\n",
      "../../Data/DATA_tidied/HDF5/C462.h5\n",
      "CAN1\n",
      "../../Data/DATA_tidied/HDF5/CAN1.h5\n",
      "CER1\n",
      "../../Data/DATA_tidied/HDF5/CER1.h5\n",
      "CFW1\n",
      "../../Data/DATA_tidied/HDF5/CFW1.h5\n",
      "CHA1\n",
      "../../Data/DATA_tidied/HDF5/CHA1.h5\n",
      "CHE1\n",
      "../../Data/DATA_tidied/HDF5/CHE1.h5\n",
      "CHP1\n",
      "../../Data/DATA_tidied/HDF5/CHP1.h5\n",
      "COZ1\n",
      "../../Data/DATA_tidied/HDF5/COZ1.h5\n",
      "CPP1\n",
      "../../Data/DATA_tidied/HDF5/CPP1.h5\n",
      "CRR1\n",
      "../../Data/DATA_tidied/HDF5/CRR1.h5\n",
      "CTM1\n",
      "../../Data/DATA_tidied/HDF5/CTM1.h5\n",
      "CWC3\n",
      "../../Data/DATA_tidied/HDF5/CWC3.h5\n",
      "CWN1\n",
      "../../Data/DATA_tidied/HDF5/CWN1.h5\n",
      "ESZ1\n",
      "../../Data/DATA_tidied/HDF5/ESZ1.h5\n",
      "FXC1\n",
      "../../Data/DATA_tidied/HDF5/FXC1.h5\n",
      "GB.C\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{k}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k\u001b[38;5;241m=\u001b[39mabbreviation)\n\u001b[1;32m     23\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m index_CM \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreepmeter_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCreepmeter_abbrv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mabbreviation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m full_name \u001b[38;5;241m=\u001b[39m creepmeter_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreepmeter_full_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[index_CM]\n\u001b[1;32m     26\u001b[0m latitude \u001b[38;5;241m=\u001b[39m creepmeter_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[index_CM]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(c2h)\n",
    "try:\n",
    "    del creeping_A\n",
    "except NameError:\n",
    "    dummy=10\n",
    "\n",
    "try:\n",
    "    del creeping_B\n",
    "except NameError:\n",
    "    dummy=10\n",
    "try:\n",
    "    del creeping_C\n",
    "except NameError:\n",
    "    dummy=10\n",
    "sac_vs_hdf5='hdf5'\n",
    "Creepmeter_dataframe_SAC = pd.DataFrame()\n",
    "for file in files:\n",
    "    # Extract the first 4 letters in uppercase\n",
    "    abbreviation = file[:4].upper()\n",
    "    print(abbreviation)\n",
    "    fn = '{k}'.format(k=abbreviation)\n",
    "    prefix = 'Slip'\n",
    "    index_CM = np.where(creepmeter_metadata['Creepmeter_abbrv']==abbreviation)[0][0]\n",
    "    full_name = creepmeter_metadata['Creepmeter_full_name'].iloc[index_CM]\n",
    "    latitude = creepmeter_metadata['Latitude'].iloc[index_CM]\n",
    "    longitude = creepmeter_metadata['Longitude'].iloc[index_CM]\n",
    "    depth = creepmeter_metadata['Depth'].iloc[index_CM]\n",
    "    length = creepmeter_metadata['Length'].iloc[index_CM]\n",
    "    obliquity = creepmeter_metadata['Obliquity'].iloc[index_CM]\n",
    "    network = creepmeter_metadata['Network'].iloc[index_CM]\n",
    "    full_name = creepmeter_metadata['Creepmeter_full_name'].iloc[index_CM]\n",
    "    \n",
    "    hdf5_path = '../../Data/DATA_tidied/HDF5/' + fn +'.h5'\n",
    "    sac_path = '../../Data/DATA_tidied/SAC/' + fn +'.SAC'\n",
    "\n",
    "    if sac_vs_hdf5 == 'hdf5':\n",
    "        path = hdf5_path\n",
    "    elif sac_vs_hdf5 == 'sac':\n",
    "        path=sac_path\n",
    "    print(path)\n",
    "    if not os.path.isfile(path):\n",
    "        data = pd.read_csv(directory + file,index_col=0)\n",
    "        try:\n",
    "            data['Time'] = pd.to_datetime(data['Time'])\n",
    "        except KeyError:\n",
    "            data['Time'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "        tm_diff = np.diff(data['Time'])/ np.timedelta64(1, 'm')\n",
    "        tm_diff = np.append(tm_diff,123456)\n",
    "        tm_diff = c2h.round_to_nearest_half(tm_diff)\n",
    "        data['tm_diff'] = tm_diff\n",
    "        data.dropna(inplace=True)\n",
    "        data['tm_diff'] = c2h.round_to_nearest_half(data['tm_diff'])\n",
    "        data.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "\n",
    "        boolarr = tm_diff <1/60\n",
    "        same_time = tm_diff[boolarr] \n",
    "\n",
    "        if len(same_time) >0:\n",
    "            datapoint_to_drop = np.where(tm_diff <1/60)[0][0]\n",
    "            data.drop(datapoint_to_drop,inplace=True)\n",
    "            data.reset_index(inplace=True)\n",
    "            data.drop(['index'],axis=1,inplace=True)\n",
    "            boolarr_diff = tm_diff >= 1/60\n",
    "            tm_diff = tm_diff[boolarr_diff]\n",
    "        data.drop(data.loc[data.Time.isnull()].index,inplace=True)\n",
    "        data.reset_index(inplace=True,drop=True) \n",
    "        data['Slip'] = data['Slip'].astype(float)\n",
    "\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(1,1,1)\n",
    "        plt.plot(tm_diff)\n",
    "        plt.ylim(0,62)\n",
    "        plt.hlines([1/60,1/6,1/2,1,2,5,10,30,60],xmin=0,xmax=len(tm_diff),colors='orange',linestyles='--',alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        allowed_modes = [1/60,1/6,1/2,1,2,5,10,30,60]\n",
    "        top_4_modes = c2h.find_top_modes(tm_diff, 4,allowed_modes)\n",
    "        print(\"Top 4 modes and their frequencies:\", top_4_modes)\n",
    "        no_mode = input('how many sampling frequencies?')\n",
    "        reversal = input('does the smaple rate revert to an earlier one? (y/n)')\n",
    "        periods = []\n",
    "        for q in range(int(no_mode)):\n",
    "            periods.append(top_4_modes[q][0])\n",
    "        print(periods)\n",
    "\n",
    "\n",
    "\n",
    "        if len(periods)==3:\n",
    "            a = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[0]), g = c2h.stringify(periods[1])))\n",
    "            b = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[1]), g = c2h.stringify(periods[0])))\n",
    "            c = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[0]), g = c2h.stringify(periods[2])))\n",
    "            d = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[2]), g = c2h.stringify(periods[0])))\n",
    "            e = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[1]), g = c2h.stringify(periods[2])))\n",
    "            f = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[2]), g = c2h.stringify(periods[1])))\n",
    "\n",
    "            indices = sorted([a,b,c,d,e,f])\n",
    "            unique_ind = np.unique(indices)\n",
    "            creeping_A = data.iloc[:unique_ind[1]]      # From start to 'a' (exclusive)\n",
    "            the_rest = data.iloc[unique_ind[1]:]\n",
    "            creeping_B = data.iloc[unique_ind[1]:unique_ind[2]]    # From 'a' (inclusive) to 'b' (exclusive)\n",
    "            creeping_C = data.iloc[unique_ind[2]:]     # From 'b' (inclusive) to 'c' (exclusive)\n",
    "            \n",
    "            creeping_A.reset_index(inplace=True,drop=True)\n",
    "            creeping_B.reset_index(inplace=True,drop=True)\n",
    "            creeping_C.reset_index(inplace=True,drop=True)\n",
    "            \n",
    "            print(creeping_A)\n",
    "            print(creeping_B)\n",
    "            print(creeping_C)\n",
    "\n",
    "\n",
    "        elif len(periods) ==2:\n",
    "            a = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[0]), g = c2h.stringify(periods[1])))\n",
    "            b = eval('c2h.{k}_to_{g}(tm_diff)'.format(k = c2h.stringify(periods[1]), g = c2h.stringify(periods[0])))\n",
    "\n",
    "            indices = sorted([a,b])\n",
    "            unique_ind = np.unique(indices)\n",
    "            creeping_A = data.iloc[:unique_ind[1]]\n",
    "            the_rest = data.iloc[unique_ind[1]:]\n",
    "            \n",
    "            if reversal == 'y':\n",
    "                the_rest.reset_index(inplace=True,drop=True)\n",
    "                a = eval('c2h.{k}_to_{g}(the_rest[\"tm_diff\"])'.format(k = c2h.stringify(periods[0]), g = c2h.stringify(periods[1])))\n",
    "                b = eval('c2h.{k}_to_{g}(the_rest[\"tm_diff\"])'.format(k = c2h.stringify(periods[1]), g = c2h.stringify(periods[0])))\n",
    "                print(a,b)\n",
    "                if abbreviation == 'R7' or abbreviation == 'RB':\n",
    "                    a = 17179\n",
    "                indices = np.unique([a,b])\n",
    "                print(indices)\n",
    "\n",
    "                creeping_B = the_rest.iloc[indices[0]:indices[1]]\n",
    "                creeping_C  = the_rest.iloc[indices[1]:]\n",
    "                \n",
    "                creeping_A = pd.concat([creeping_A,creeping_C],ignore_index=True)\n",
    "                creeping_B.reset_index(inplace=True,drop=True)\n",
    "                del creeping_C\n",
    "\n",
    "            else:\n",
    "                creeping_B = data.iloc[unique_ind[1]:]\n",
    "                creeping_A.reset_index(inplace=True,drop=True)\n",
    "                creeping_B.reset_index(inplace=True,drop=True)\n",
    "\n",
    "            print(creeping_A)\n",
    "            print(creeping_B)\n",
    "\n",
    "        elif len(periods) == 1:\n",
    "            creeping_A = data.iloc[:]\n",
    "            print(creeping_A)\n",
    "        \n",
    "        \n",
    "        Number_of_splits = []\n",
    "        Sampling_rates_variable = []\n",
    "        try:\n",
    "            sampling_A = c2h.find_top_modes(creeping_A['tm_diff'],1,allowed_modes)[0][0]\n",
    "            Sampling_rates_variable.append(sampling_A)\n",
    "            Number_of_splits.append('A')\n",
    "        except (IndexError, NameError, KeyError):\n",
    "            sampling_A = 0\n",
    "\n",
    "        try:\n",
    "            sampling_B = c2h.find_top_modes(creeping_B['tm_diff'],1,allowed_modes)[0][0]\n",
    "            Sampling_rates_variable.append(sampling_B)\n",
    "            Number_of_splits.append('B')\n",
    "        except (IndexError, NameError, KeyError):\n",
    "            sampling_B = 0\n",
    "\n",
    "        try:\n",
    "            sampling_C = c2h.find_top_modes(creeping_C['tm_diff'],1,allowed_modes)[0][0]\n",
    "            Sampling_rates_variable.append(sampling_C)\n",
    "            Number_of_splits.append('C')\n",
    "        except (IndexError, NameError, KeyError):\n",
    "            sampling_C = 0\n",
    "\n",
    "        print(Sampling_rates_variable,Number_of_splits)\n",
    "\n",
    "        if sac_vs_hdf5 == 'sac':\n",
    "            for i in range(len(Number_of_splits)):\n",
    "                print('interpolating creeping_{k}'.format(k=Number_of_splits[i]))\n",
    "                upsampled = eval('creeping_{k}'.format(k=Number_of_splits[i]))\n",
    "                val1_okay = math.isnan(upsampled.Slip.iloc[0])\n",
    "                if val1_okay == True:\n",
    "                    upsampled.reset_index(inplace=True)\n",
    "                    upsampled.drop([0],axis=0,inplace=True)\n",
    "                tr = obspy.Trace(np.array(upsampled.Slip))\n",
    "                st = obspy.Stream(tr)\n",
    "                st[0].stats.network = network\n",
    "                st[0].stats.station = abbreviation\n",
    "                st[0].stats.location = '00'\n",
    "                st[0].stats.channel = 'slip'\n",
    "                st[0].stats.starttime = pd.to_datetime(upsampled.Time.iloc[0])\n",
    "                st[0].stats.delta = 60*Sampling_rates_variable[i]\n",
    "                #st[0].plot()\n",
    "                instrument = pd.DataFrame({'Network':[st[0].stats.network],'Creepmeter_full_name':['{k}'.format(k=full_name)],\n",
    "                                        'Creepmeter_abbrv':['{k}'.format(k=abbreviation)],'File_code':['{p}_{q}'.format(p=abbreviation,q=i)],\n",
    "                                        'Start Time':[st[0].stats.starttime],'End Time':[st[0].stats.endtime],\n",
    "                                        'Sampling rate, Hz':[st[0].stats.sampling_rate],'Sampling rate, mins':(1/st[0].stats.sampling_rate)/60,\n",
    "                                        'Latitude':[latitude],'Longitude':[longitude]})\n",
    "                \n",
    "                Creepmeter_dataframe_SAC = pd.concat([Creepmeter_dataframe_SAC,instrument],ignore_index=True)\n",
    "                Creepmeter_dataframe_SAC.drop_duplicates(subset='File_code',inplace=True)\n",
    "                Creepmeter_dataframe_SAC.reset_index(inplace=True,drop=True)\n",
    "                \n",
    "                Creepmeter_dataframe_SAC.to_csv('../../Data/DATA_tidied/creepmeter_metadata_post_standardisation_sac_codes.csv')\n",
    "                path = '../../Data/DATA_tidied/SAC/'\n",
    "                st.write(path +'{p}_{q}.SAC'.format(p=abbreviation,q=i),format='SAC')\n",
    "                st[0].plot();\n",
    "            print(Creepmeter_dataframe_SAC)\n",
    "            try:\n",
    "                del creeping_A\n",
    "            except NameError:\n",
    "                dummy=10\n",
    "\n",
    "            try:\n",
    "                del creeping_B\n",
    "            except NameError:\n",
    "                dummy=10\n",
    "            try:\n",
    "                del creeping_C\n",
    "            except NameError:\n",
    "                dummy=10\n",
    "        \n",
    "        elif sac_vs_hdf5 =='hdf5':\n",
    "            with h5py.File(path, 'w') as f:\n",
    "                f.attrs['author'] = 'Daniel B. Gittins'\n",
    "                f.attrs['network'] = network\n",
    "                f.attrs['latitude'] = latitude\n",
    "                f.attrs['longitude'] = longitude\n",
    "                f.attrs['depth'] = depth\n",
    "                f.attrs['length'] = length\n",
    "                f.attrs['obliquity'] = obliquity\n",
    "                # Create a group to store both waves together\n",
    "                for i in range(len(Number_of_splits)):\n",
    "                    smpl_rate = Sampling_rates_variable[i]\n",
    "                    group_name = fn+'_{k}mins'.format(k=smpl_rate)\n",
    "                    time_data = eval('creeping_{k}.Time'.format(k=Number_of_splits[i]))\n",
    "                    # Assuming 'data['Time']' is your pandas Series of datetime objects\n",
    "                    datetime_strings = time_data.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "\n",
    "                    dependent_data = eval('creeping_{k}.Slip'.format(k=Number_of_splits[i])).tolist()\n",
    "                    group = f.create_group(group_name)\n",
    "                    # Create datasets for time and dependent variable in the group\n",
    "                    group.create_dataset('Time_{p}_{k}mins'.format(p=abbreviation,k=smpl_rate), data=datetime_strings)\n",
    "                    group.create_dataset('Slip_{p}_{k}mins'.format(p=abbreviation,k=smpl_rate), data=dependent_data)\n",
    "                \n",
    "                    \n",
    "                    # Store metadata about the dependent variable\n",
    "                    if Sampling_rates_variable[i] >=1:\n",
    "                        description = f\"Slip for {abbreviation} at {smpl_rate} minute sampling\"\n",
    "                    else:\n",
    "                        description = f\"Slip for {abbreviation} at {smpl_rate*60} second sampling\"\n",
    "                    group.attrs['description'] = description\n",
    "\n",
    "                    \n",
    "                    # Optional: Store additional metadata (e.g., units, sampling rate)\n",
    "                    starttime = eval('creeping_{k}.Time.iloc[0]'.format(k=Number_of_splits[i]))\n",
    "                    endtime = eval('creeping_{k}.Time.iloc[-1]'.format(k=Number_of_splits[i]))\n",
    "                    group.attrs['sampling_rate'] = Sampling_rates_variable[i]  # Example: 100 Hz sampling rate (10 ms interval)\n",
    "                    group.attrs['time_units'] = 'minutes'     # Time units\n",
    "                    group.attrs['slip_units'] = 'millimetres'  # Dependent variable units (e.g., for slip)\n",
    "                    group.attrs['channel'] = 'slip'\n",
    "                    group.attrs['starttime'] = starttime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    group.attrs['delta'] = 60*Sampling_rates_variable[i]\n",
    "                    instrument = pd.DataFrame({'Network':[network],'Creepmeter_full_name':[full_name],'Creepmeter_abbrv':[abbreviation],'File_code':[fn],\n",
    "                                            'Start Time':[starttime],'End Time':[endtime],'Sampling rate, mins':[smpl_rate],\n",
    "                                            'Latitude':[latitude],'Longitude':[longitude],'Description':[description],'Depth':[depth],'Length':[length],\n",
    "                                            'Obliquity':[obliquity],'group_name':group_name})\n",
    "                    Creepmeter_dataframe = pd.concat([Creepmeter_dataframe,instrument],ignore_index=True)\n",
    "                try:\n",
    "                    del creeping_A\n",
    "                except NameError:\n",
    "                    dummy=10\n",
    "\n",
    "                try:\n",
    "                    del creeping_B\n",
    "                except NameError:\n",
    "                    dummy=10\n",
    "                try:\n",
    "                    del creeping_C\n",
    "                except NameError:\n",
    "                    dummy=10\n",
    "            print('HDF5 structure')\n",
    "            c2h.print_hdf5_structure('../../Data/DATA_tidied/HDF5/' + fn +'.h5')\n",
    "            Creepmeter_dataframe.drop_duplicates()\n",
    "            Creepmeter_dataframe.reset_index(inplace=True,drop=True)\n",
    "            Creepmeter_dataframe.to_csv('../../Data/DATA_tidied/creepmeter_metadata_post_standardisation.csv')\n",
    "            print('Creepmeter_meta_data')\n",
    "            print(Creepmeter_dataframe)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temp = pd.read_csv('../../Data/DATA_tidied/CSV/Temperature/Balikburnu_Temp.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temp['Date'] = pd.to_datetime(Temp['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation = 'BAL1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = Temp['Date']\n",
    "datetime_strings = time_data.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "dependent_data = Temp['degC'].tolist()\n",
    "hdf5_path = '../../Data/DATA_tidied/HDF5/{k}.h5'.format(k=abbreviation)\n",
    "with h5py.File(hdf5_path, 'a') as f:\n",
    "    del f['Temperature']\n",
    "    group = f.create_group('Temperature')\n",
    "    # Create datasets for time and dependent variable in the group\n",
    "    group.create_dataset('Time_{p}'.format(p=abbreviation), data=datetime_strings)\n",
    "    group.create_dataset('Temperature_{p}'.format(p=abbreviation), data=dependent_data)\n",
    "    group.attrs['Units'] = 'Celsius'\n",
    "print('HDF5 structure')\n",
    "c2h.print_hdf5_structure(hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily = pd.read_csv('../../Data/DATA_tidied/CSV/Daily/xva1_day.csv',index_col=0)\n",
    "Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation = 'XVA1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_strings = Daily['Time'].tolist()\n",
    "dependent_data = Daily['Slip'].tolist()\n",
    "hdf5_path = '../../Data/DATA_tidied/HDF5/{k}.h5'.format(k=abbreviation)\n",
    "with h5py.File(hdf5_path, 'a') as f:\n",
    "    #del f['Daily measurements']\n",
    "    group = f.create_group('Daily_measurements')\n",
    "    # Create datasets for time and dependent variable in the group\n",
    "    group.create_dataset('Time_{p}_daily'.format(p=abbreviation), data=datetime_strings)\n",
    "    group.create_dataset('Slip_{p}_daily'.format(p=abbreviation), data=dependent_data)\n",
    "    group.attrs['Units_slip'] =  'millimetres'\n",
    "    group.attrs['Start date'] = datetime_strings[0]\n",
    "    group.attrs['End date'] = datetime_strings[-1]\n",
    "print('HDF5 structure')\n",
    "c2h.print_hdf5_structure(hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Manual = pd.read_csv('../../Data/DATA_tidied/CSV/Manual/bit1_man.csv',index_col=0)\n",
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation = 'BIT1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_strings = Manual['Time'].tolist()\n",
    "dependent_data = Manual['Slip'].tolist()\n",
    "hdf5_path = '../../Data/DATA_tidied/HDF5/{k}.h5'.format(k=abbreviation)\n",
    "with h5py.File(hdf5_path, 'a') as f:\n",
    "    del f['Manual_measurements']\n",
    "    group = f.create_group('Manual_measurements')\n",
    "    # Create datasets for time and dependent variable in the group\n",
    "    group.create_dataset('Time_{p}_Manual'.format(p=abbreviation), data=datetime_strings)\n",
    "    group.create_dataset('Slip_{p}_Manual'.format(p=abbreviation), data=dependent_data)\n",
    "    group.attrs['Units_slip'] =  'millimetres'\n",
    "    group.attrs['Start date'] = datetime_strings[0]\n",
    "    group.attrs['End date'] = datetime_strings[-1]\n",
    "print('HDF5 structure')\n",
    "c2h.print_hdf5_structure(hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ortho = pd.read_csv('../../Data/DATA_tidied/CSV/Orthogonal/Karakose_Orthogonal.csv',index_col=0)\n",
    "Ortho['Date'] = pd.to_datetime(Ortho['Date'])\n",
    "tm_diff = np.diff(Ortho['Date'])/ np.timedelta64(1, 'm')\n",
    "Ortho['diff'] = np.append(tm_diff,123456)\n",
    "Ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation = 'KAR1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = Ortho['Date']\n",
    "datetime_strings = time_data.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "dependent_data = Ortho['Ortho'].tolist()\n",
    "hdf5_path = '../../Data/DATA_tidied/HDF5/{k}.h5'.format(k=abbreviation)\n",
    "with h5py.File(hdf5_path, 'a') as f:\n",
    "    #del f['Orthogonal']\n",
    "    group = f.create_group('Orthogonal')\n",
    "    # Create datasets for time and dependent variable in the group\n",
    "    group.create_dataset('Time_{p}_Orthogonal'.format(p=abbreviation), data=datetime_strings)\n",
    "    group.create_dataset('Slip_{p}_Orthogonal'.format(p=abbreviation), data=dependent_data)\n",
    "    group.attrs['Units_Orthogonal'] =  'millimetres'\n",
    "    group.attrs['Start date'] = datetime_strings[0]\n",
    "    group.attrs['End date'] = datetime_strings[-1]\n",
    "print('HDF5 structure')\n",
    "c2h.print_hdf5_structure(hdf5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 to SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: CWN1_1.0mins\n",
      "Data: <HDF5 group \"/CWN1_1.0mins\" (2 members)>\n",
      "Slip: [389.44025 389.43839 389.43839 ... 424.91962 424.91962 424.92059]\n",
      "Time: DatetimeIndex(['2020-09-22 13:52:00', '2020-09-22 13:52:59',\n",
      "               '2020-09-22 13:54:00', '2020-09-22 13:54:59',\n",
      "               '2020-09-22 13:56:00', '2020-09-22 13:57:00',\n",
      "               '2020-09-22 13:57:59', '2020-09-22 13:59:00',\n",
      "               '2020-09-22 13:59:59', '2020-09-22 14:01:00',\n",
      "               ...\n",
      "               '2023-12-15 12:51:00', '2023-12-15 12:51:59',\n",
      "               '2023-12-15 12:53:00', '2023-12-15 12:54:00',\n",
      "               '2023-12-15 12:54:59', '2023-12-15 12:56:00',\n",
      "               '2023-12-15 12:56:59', '2023-12-15 12:58:00',\n",
      "               '2023-12-15 12:58:59', '2023-12-15 13:00:00'],\n",
      "              dtype='datetime64[ns]', length=1573217, freq=None)\n",
      "<Attributes of HDF5 object at 140592647257872>\n",
      "Key: CWN1_10.0mins\n",
      "CWN1_10.0mins not a key, if daily or manual do not worry.\n",
      "Data: <HDF5 group \"/CWN1_10.0mins\" (2 members)>\n",
      "Slip: [  4.05      4.07      4.07    ... 389.39185 389.39185 389.39185]\n",
      "Time: DatetimeIndex(['2020-09-22 13:52:00', '2020-09-22 13:52:59',\n",
      "               '2020-09-22 13:54:00', '2020-09-22 13:54:59',\n",
      "               '2020-09-22 13:56:00', '2020-09-22 13:57:00',\n",
      "               '2020-09-22 13:57:59', '2020-09-22 13:59:00',\n",
      "               '2020-09-22 13:59:59', '2020-09-22 14:01:00',\n",
      "               ...\n",
      "               '2023-12-15 12:51:00', '2023-12-15 12:51:59',\n",
      "               '2023-12-15 12:53:00', '2023-12-15 12:54:00',\n",
      "               '2023-12-15 12:54:59', '2023-12-15 12:56:00',\n",
      "               '2023-12-15 12:56:59', '2023-12-15 12:58:00',\n",
      "               '2023-12-15 12:58:59', '2023-12-15 13:00:00'],\n",
      "              dtype='datetime64[ns]', length=1573217, freq=None)\n",
      "<Attributes of HDF5 object at 140590703956256>\n",
      "Key: CWN1_2.0mins\n",
      "CWN1_2.0mins not a key, if daily or manual do not worry.\n",
      "Data: <HDF5 group \"/CWN1_2.0mins\" (2 members)>\n",
      "Slip: [389.33594 389.33408 389.33594 ... 389.44025 389.44025 389.43933]\n",
      "Time: DatetimeIndex(['2020-09-22 13:52:00', '2020-09-22 13:52:59',\n",
      "               '2020-09-22 13:54:00', '2020-09-22 13:54:59',\n",
      "               '2020-09-22 13:56:00', '2020-09-22 13:57:00',\n",
      "               '2020-09-22 13:57:59', '2020-09-22 13:59:00',\n",
      "               '2020-09-22 13:59:59', '2020-09-22 14:01:00',\n",
      "               ...\n",
      "               '2023-12-15 12:51:00', '2023-12-15 12:51:59',\n",
      "               '2023-12-15 12:53:00', '2023-12-15 12:54:00',\n",
      "               '2023-12-15 12:54:59', '2023-12-15 12:56:00',\n",
      "               '2023-12-15 12:56:59', '2023-12-15 12:58:00',\n",
      "               '2023-12-15 12:58:59', '2023-12-15 13:00:00'],\n",
      "              dtype='datetime64[ns]', length=1573217, freq=None)\n",
      "<Attributes of HDF5 object at 140590749358240>\n",
      "Key: Daily_measurements\n",
      "Daily_measurements not a key, if daily or manual do not worry.\n",
      "Data: <HDF5 group \"/Daily_measurements\" (2 members)>\n",
      "Slip: [389.33594 389.33408 389.33594 ... 389.44025 389.44025 389.43933]\n",
      "Time: DatetimeIndex(['2020-09-22 13:52:00', '2020-09-22 13:52:59',\n",
      "               '2020-09-22 13:54:00', '2020-09-22 13:54:59',\n",
      "               '2020-09-22 13:56:00', '2020-09-22 13:57:00',\n",
      "               '2020-09-22 13:57:59', '2020-09-22 13:59:00',\n",
      "               '2020-09-22 13:59:59', '2020-09-22 14:01:00',\n",
      "               ...\n",
      "               '2023-12-15 12:51:00', '2023-12-15 12:51:59',\n",
      "               '2023-12-15 12:53:00', '2023-12-15 12:54:00',\n",
      "               '2023-12-15 12:54:59', '2023-12-15 12:56:00',\n",
      "               '2023-12-15 12:56:59', '2023-12-15 12:58:00',\n",
      "               '2023-12-15 12:58:59', '2023-12-15 13:00:00'],\n",
      "              dtype='datetime64[ns]', length=1573217, freq=None)\n",
      "<Attributes of HDF5 object at 140592600348400>\n",
      "Key: Manual_measurements\n",
      "Manual_measurements not a key, if daily or manual do not worry.\n",
      "Data: <HDF5 group \"/Manual_measurements\" (2 members)>\n",
      "Slip: [389.33594 389.33408 389.33594 ... 389.44025 389.44025 389.43933]\n",
      "Time: DatetimeIndex(['2020-09-22 13:52:00', '2020-09-22 13:52:59',\n",
      "               '2020-09-22 13:54:00', '2020-09-22 13:54:59',\n",
      "               '2020-09-22 13:56:00', '2020-09-22 13:57:00',\n",
      "               '2020-09-22 13:57:59', '2020-09-22 13:59:00',\n",
      "               '2020-09-22 13:59:59', '2020-09-22 14:01:00',\n",
      "               ...\n",
      "               '2023-12-15 12:51:00', '2023-12-15 12:51:59',\n",
      "               '2023-12-15 12:53:00', '2023-12-15 12:54:00',\n",
      "               '2023-12-15 12:54:59', '2023-12-15 12:56:00',\n",
      "               '2023-12-15 12:56:59', '2023-12-15 12:58:00',\n",
      "               '2023-12-15 12:58:59', '2023-12-15 13:00:00'],\n",
      "              dtype='datetime64[ns]', length=1573217, freq=None)\n",
      "<Attributes of HDF5 object at 140590703956256>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is for importing into Igor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragma TextEncoding = \"UTF-8\"\n",
    "#pragma rtGlobals=3\n",
    "#pragma DefaultTab={3,20,4}\n",
    "\n",
    "// Creates a date/time wave from a date wave stored as text in YYYY-MM-DD format\n",
    "// and a time wave stored as text in HH:MM:YY format.\n",
    "// Example:\n",
    "//      Make/O/T wDateAsText = {\"2016-08-15\", \"2016-08-15\"}\n",
    "//      Make/O/T wTimeAsText = {\"19:01:00\", \"19:01:18\"}\n",
    "//      TextWavesToDateTimeWave(wDateAsText, wTimeAsText, \"wDateTime\")\n",
    "//      Edit wDateTime\n",
    "\n",
    "Function ConvertTextToDateTime(dateTimeAsText)\n",
    "    String dateTimeAsText       // Assumed in YYYY-MM-DD HH:MM:SS format\n",
    "    \n",
    "    Variable dt\n",
    "    Variable year, month, day\n",
    "    Variable hour, minute, second\n",
    "\n",
    "    // Use sscanf to parse the full datetime string\n",
    "    sscanf dateTimeAsText, \"%d-%d-%d %d:%d:%d\", year, month, day, hour, minute, second\n",
    "    \n",
    "    // Convert the date part to seconds since the epoch\n",
    "    dt = Date2Secs(year, month, day)\n",
    "    \n",
    "    // Calculate the time of day in seconds\n",
    "    Variable timeOfDay = 3600 * hour + 60 * minute + second\n",
    "    \n",
    "    // Add the time of day to the date\n",
    "    dt += timeOfDay\n",
    "    \n",
    "    return dt\n",
    "End\n",
    "\n",
    "\n",
    "Function/WAVE TextWavesToDateTimeWave(dateTimeAsTextWave, outputWaveName)\n",
    "    WAVE/T dateTimeAsTextWave       // Assumed in YYYY-MM-DD HH:MM:SS format\n",
    "    String outputWaveName\n",
    "\n",
    "    Variable numPoints = numpnts(dateTimeAsTextWave)\n",
    "    Make/O/D/N=(numPoints) $outputWaveName\n",
    "    WAVE wOut = $outputWaveName\n",
    "    SetScale d, 0, 0, \"dat\", wOut\n",
    "   \n",
    "    Variable i\n",
    "    for(i=0; i<numPoints; i+=1)\n",
    "        String dateTimeAsText = dateTimeAsTextWave[i]\n",
    "        Variable dt = ConvertTextToDateTime(dateTimeAsText)\n",
    "        wOut[i] = dt   \n",
    "    endfor \n",
    "\n",
    "    return wOut\n",
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creepmeters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
